{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11873364,"sourceType":"datasetVersion","datasetId":7461802},{"sourceId":11944092,"sourceType":"datasetVersion","datasetId":7494136}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchaudio\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport glob\nfrom typing import List, Tuple\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:20.281663Z","iopub.execute_input":"2025-05-25T17:58:20.281842Z","iopub.status.idle":"2025-05-25T17:58:22.787175Z","shell.execute_reply.started":"2025-05-25T17:58:20.281826Z","shell.execute_reply":"2025-05-25T17:58:22.785297Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    \"\"\"Dataset for loading and preprocessing audio files\"\"\"\n    \n    def __init__(\n        self, \n        audio_dir: str,\n        sample_rate: int = 24000,\n        segment_length: float = 4.0,  # seconds\n        file_extensions: List[str] = ['.mp3', '.wav', '.flac']\n    ):\n        self.audio_dir = Path(audio_dir)\n        self.sample_rate = sample_rate\n        self.segment_samples = int(segment_length * sample_rate)\n        \n        # Find all audio files\n        self.audio_files = []\n        for ext in file_extensions:\n            self.audio_files.extend(glob.glob(str(self.audio_dir / f\"**/*{ext}\"), recursive=True))\n        \n        print(f\"Found {len(self.audio_files)} audio files\")\n        \n    def __len__(self):\n        return len(self.audio_files)\n    \n    def __getitem__(self, idx):\n        audio_path = self.audio_files[idx]\n        \n        try:\n            # Load audio\n            waveform, orig_sr = torchaudio.load(audio_path)\n            \n            # Resample if needed\n            if orig_sr != self.sample_rate:\n                resampler = torchaudio.transforms.Resample(orig_sr, self.sample_rate)\n                waveform = resampler(waveform)\n            \n            # Convert to mono if stereo\n            if waveform.shape[0] > 1:\n                waveform = torch.mean(waveform, dim=0, keepdim=True)\n            \n            # Random crop to fixed length\n            if waveform.shape[1] > self.segment_samples:\n                start_idx = torch.randint(0, waveform.shape[1] - self.segment_samples, (1,))\n                waveform = waveform[:, start_idx:start_idx + self.segment_samples]\n            elif waveform.shape[1] < self.segment_samples:\n                # Pad if too short\n                pad_length = self.segment_samples - waveform.shape[1]\n                waveform = F.pad(waveform, (0, pad_length))\n            \n            # Normalize - FIXED: Handle edge case where max is 0\n            max_val = torch.max(torch.abs(waveform))\n            if max_val > 0:\n                waveform = waveform / max_val\n            \n            return waveform\n            \n        except Exception as e:\n            print(f\"Error loading {audio_path}: {e}\")\n            # Return a zero tensor if loading fails\n            return torch.zeros(1, self.segment_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.789228Z","iopub.execute_input":"2025-05-25T17:58:22.789754Z","iopub.status.idle":"2025-05-25T17:58:22.803135Z","shell.execute_reply.started":"2025-05-25T17:58:22.789721Z","shell.execute_reply":"2025-05-25T17:58:22.802364Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ConvEncoder(nn.Module):\n    \"\"\"\n    Convolutional encoder that downsamples audio and extracts features\n    Similar to DAC encoder structure\n    \"\"\"\n    def __init__(self, dim: int = 64):\n        super().__init__()\n        # Progressive downsampling with increasing channels\n        self.layers = nn.ModuleList([\n            nn.Conv1d(1, dim, kernel_size=7, stride=1, padding=3),\n            nn.Conv1d(dim, dim, kernel_size=3, stride=2, padding=1),    # /2\n            nn.Conv1d(dim, dim*2, kernel_size=3, stride=2, padding=1),  # /4  \n            nn.Conv1d(dim*2, dim*4, kernel_size=3, stride=2, padding=1), # /8\n            nn.Conv1d(dim*4, dim*8, kernel_size=3, stride=2, padding=1), # /16\n        ])\n        \n        # FIXED: Use GroupNorm instead of BatchNorm for better stability\n        self.norms = nn.ModuleList([\n            nn.GroupNorm(4, dim),\n            nn.GroupNorm(4, dim), \n            nn.GroupNorm(8, dim*2),\n            nn.GroupNorm(16, dim*4),\n            nn.GroupNorm(32, dim*8),\n        ])\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for layer, norm in zip(self.layers, self.norms):\n            x = layer(x)\n            x = norm(x)\n            x = F.leaky_relu(x, 0.2)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.804022Z","iopub.execute_input":"2025-05-25T17:58:22.804289Z","iopub.status.idle":"2025-05-25T17:58:22.822968Z","shell.execute_reply.started":"2025-05-25T17:58:22.804271Z","shell.execute_reply":"2025-05-25T17:58:22.822213Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class VectorQuantizer(nn.Module):\n    \"\"\"\n    Single layer Vector Quantizer\n    Maps continuous features to discrete codebook entries\n    \"\"\"\n    def __init__(self, codebook_size: int, codebook_dim: int, beta: float = 0.25):\n        super().__init__()\n        self.codebook_size = codebook_size\n        self.codebook_dim = codebook_dim\n        self.beta = beta\n        \n        # Initialize codebook with random vectors\n        self.codebook = nn.Embedding(codebook_size, codebook_dim)\n        self.codebook.weight.data.uniform_(-1/codebook_size, 1/codebook_size)\n        \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        batch_size, dim, time = x.shape\n        x_flat = x.permute(0, 2, 1).contiguous().view(-1, dim)\n        \n        # IMPROVED: More efficient distance calculation\n        distances = torch.cdist(x_flat, self.codebook.weight)\n        indices = torch.argmin(distances, dim=1)\n        quantized_flat = self.codebook(indices)\n        \n        quantized = quantized_flat.view(batch_size, time, dim).permute(0, 2, 1)\n        indices = indices.view(batch_size, time)\n        \n        # VQ losses\n        codebook_loss = F.mse_loss(quantized_flat, x_flat.detach())\n        commitment_loss = F.mse_loss(x_flat, quantized_flat.detach())\n        vq_loss = codebook_loss + self.beta * commitment_loss\n        \n        # Straight-through estimator\n        quantized = x + (quantized - x).detach()\n        \n        return quantized, indices, vq_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.823648Z","iopub.execute_input":"2025-05-25T17:58:22.823856Z","iopub.status.idle":"2025-05-25T17:58:22.838061Z","shell.execute_reply.started":"2025-05-25T17:58:22.823840Z","shell.execute_reply":"2025-05-25T17:58:22.837423Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ResidualVectorQuantizer(nn.Module):\n    \"\"\"\n    Residual Vector Quantizer with multiple layers\n    Each layer quantizes the residual from previous layers\n    \"\"\"\n    def __init__(\n        self, \n        input_dim: int = 512,  # FIXED: Make input dimension configurable\n        n_layers: int = 12,\n        codebook_size: int = 1024, \n        codebook_dim: int = 8,\n        beta: float = 0.25\n    ):\n        super().__init__()\n        self.n_layers = n_layers\n        self.codebook_size = codebook_size\n        self.codebook_dim = codebook_dim\n        \n        # Create multiple VQ layers\n        self.quantizers = nn.ModuleList([\n            VectorQuantizer(codebook_size, codebook_dim, beta) \n            for _ in range(n_layers)\n        ])\n        \n        # Project encoder features to codebook dimension\n        self.input_proj = nn.Conv1d(input_dim, codebook_dim, kernel_size=1)\n        \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor], torch.Tensor]:\n        x = self.input_proj(x)\n        quantized = torch.zeros_like(x)\n        residual = x\n        all_indices = []\n        total_loss = 0.0\n        \n        for quantizer in self.quantizers:\n            q, indices, loss = quantizer(residual)\n            quantized += q\n            residual -= q\n            all_indices.append(indices)\n            total_loss += loss\n        \n        return quantized, all_indices, total_loss\n    \n    def encode(self, x: torch.Tensor) -> List[torch.Tensor]:\n        with torch.no_grad():\n            _, indices, _ = self.forward(x)\n        return indices\n    \n    def decode(self, indices_list: List[torch.Tensor]) -> torch.Tensor:\n        quantized = None\n        for i, indices in enumerate(indices_list):\n            q = self.quantizers[i].codebook(indices)\n            q = q.permute(0, 2, 1)\n            quantized = q if quantized is None else quantized + q\n        return quantized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.838787Z","iopub.execute_input":"2025-05-25T17:58:22.839070Z","iopub.status.idle":"2025-05-25T17:58:22.851524Z","shell.execute_reply.started":"2025-05-25T17:58:22.839050Z","shell.execute_reply":"2025-05-25T17:58:22.850858Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class VocosDecoder(nn.Module):\n    \"\"\"\n    Vocos-style decoder for efficient audio reconstruction\n    Uses transposed convolutions to upsample back to original resolution\n    \"\"\"\n    def __init__(self, input_dim: int = 8, dim: int = 64):\n        super().__init__()\n        \n        # FIXED: Match input dimension from quantizer\n        self.input_proj = nn.Conv1d(input_dim, dim*8, kernel_size=1)\n        \n        # Progressive upsampling with decreasing channels\n        self.layers = nn.ModuleList([\n            nn.ConvTranspose1d(dim*8, dim*4, kernel_size=4, stride=2, padding=1),\n            nn.ConvTranspose1d(dim*4, dim*2, kernel_size=4, stride=2, padding=1),\n            nn.ConvTranspose1d(dim*2, dim, kernel_size=4, stride=2, padding=1),\n            nn.ConvTranspose1d(dim, dim, kernel_size=4, stride=2, padding=1),\n            nn.Conv1d(dim, 1, kernel_size=7, stride=1, padding=3),\n        ])\n        \n        # FIXED: Use GroupNorm for better stability\n        self.norms = nn.ModuleList([\n            nn.GroupNorm(16, dim*4),\n            nn.GroupNorm(8, dim*2), \n            nn.GroupNorm(4, dim),\n            nn.GroupNorm(4, dim),\n        ])\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.input_proj(x)\n        \n        for i, layer in enumerate(self.layers[:-1]):\n            x = layer(x)\n            x = self.norms[i](x)\n            x = F.leaky_relu(x, 0.2)\n        \n        x = self.layers[-1](x)\n        return torch.tanh(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.852394Z","iopub.execute_input":"2025-05-25T17:58:22.853072Z","iopub.status.idle":"2025-05-25T17:58:22.866224Z","shell.execute_reply.started":"2025-05-25T17:58:22.853049Z","shell.execute_reply":"2025-05-25T17:58:22.865487Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AcousticCodec(nn.Module):\n    \"\"\"\n    Acoustic Codec for converting audio waveform to discrete tokens and back\n    Based on DAC (Descript Audio Codec) with Vocos decoder\n    \"\"\"\n    def __init__(\n        self,\n        sample_rate: int = 24000,\n        n_layers: int = 12,           # RVQ layers\n        codebook_size: int = 1024,    # entries per codebook\n        codebook_dim: int = 8,        # dimension of each entry\n        encoder_dim: int = 64,        # base encoder dimension\n        decoder_dim: int = 64,        # base decoder dimension\n    ):\n        super().__init__()\n        \n        self.sample_rate = sample_rate\n        self.n_layers = n_layers\n        self.codebook_size = codebook_size\n        self.codebook_dim = codebook_dim\n        \n        # Main components\n        self.encoder = ConvEncoder(encoder_dim)\n        \n        # FIXED: Pass correct input dimension to RVQ\n        encoder_output_dim = encoder_dim * 8  # From ConvEncoder final layer\n        self.quantizer = ResidualVectorQuantizer(\n            input_dim=encoder_output_dim,\n            n_layers=n_layers,\n            codebook_size=codebook_size,\n            codebook_dim=codebook_dim\n        )\n        \n        self.decoder = VocosDecoder(\n            input_dim=codebook_dim,\n            dim=decoder_dim\n        )\n    \n    def encode(self, audio: torch.Tensor) -> List[torch.Tensor]:\n        \"\"\"\n        Encode audio to discrete tokens\n        \n        Args:\n            audio: (batch, 1, time) - Raw audio waveform\n            \n        Returns:\n            tokens: List of (batch, time_compressed) - Discrete tokens for each layer\n        \"\"\"\n        features = self.encoder(audio)\n        tokens = self.quantizer.encode(features)\n        return tokens\n    \n    def decode(self, tokens: List[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Decode tokens back to audio\n        \n        Args:\n            tokens: List of (batch, time_compressed) - Discrete tokens for each layer\n            \n        Returns:\n            audio: (batch, 1, time) - Reconstructed audio\n        \"\"\"\n        quantized = self.quantizer.decode(tokens)\n        audio = self.decoder(quantized)\n        return audio\n    \n    def forward(self, audio: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Full encode-decode cycle\n        \n        Returns:\n            reconstructed_audio: Reconstructed audio\n            tokens: List of discrete tokens for each RVQ layer  \n            vq_loss: Loss for training quantizer\n        \"\"\"\n        # Encode to features\n        features = self.encoder(audio)\n        \n        # Quantize features\n        quantized, tokens, vq_loss = self.quantizer(features)\n        \n        # Decode to audio\n        reconstructed = self.decoder(quantized)\n        \n        return reconstructed, tokens, vq_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.867006Z","iopub.execute_input":"2025-05-25T17:58:22.867219Z","iopub.status.idle":"2025-05-25T17:58:22.882189Z","shell.execute_reply.started":"2025-05-25T17:58:22.867204Z","shell.execute_reply":"2025-05-25T17:58:22.881514Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# IMPROVED: Simpler training function\ndef train_codec(\n    audio_dir: str,\n    num_epochs: int = 10,\n    batch_size: int = 8,\n    learning_rate: float = 1e-4,\n    sample_rate: int = 24000,\n    segment_length: float = 4.0,\n    save_path: str = \"acoustic_codec.pth\"\n):\n    \"\"\"\n    Simplified training function for the acoustic codec\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Create dataset and dataloader\n    dataset = AudioDataset(\n        audio_dir=audio_dir,\n        sample_rate=sample_rate,\n        segment_length=segment_length\n    )\n    \n    if len(dataset) == 0:\n        raise ValueError(f\"No audio files found in {audio_dir}\")\n    \n    dataloader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,  # Reduced for stability\n        pin_memory=True if device.type == 'cuda' else False\n    )\n    \n    # Create model\n    codec = AcousticCodec(sample_rate=sample_rate).to(device)\n    optimizer = optim.Adam(codec.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n    \n    print(f\"Model parameters: {sum(p.numel() for p in codec.parameters()):,}\")\n    \n    # Training loop\n    codec.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        total_rec_loss = 0\n        total_vq_loss = 0\n        \n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        for batch_idx, audio in enumerate(progress_bar):\n            audio = audio.to(device)\n            \n            # Forward pass\n            reconstructed_audio, tokens, vq_loss = codec(audio)\n            \n            # Losses\n            reconstruction_loss = F.l1_loss(reconstructed_audio, audio)\n            loss = reconstruction_loss + 0.1 * vq_loss  # Weight VQ loss lower\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # ADDED: Gradient clipping for stability\n            torch.nn.utils.clip_grad_norm_(codec.parameters(), 1.0)\n            \n            optimizer.step()\n            \n            # Track losses\n            total_loss += loss.item()\n            total_rec_loss += reconstruction_loss.item()\n            total_vq_loss += vq_loss.item()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\n                'Loss': f\"{loss.item():.4f}\",\n                'Rec': f\"{reconstruction_loss.item():.4f}\",\n                'VQ': f\"{vq_loss.item():.4f}\"\n            })\n        \n        scheduler.step()\n        \n        # Print epoch summary\n        avg_loss = total_loss / len(dataloader)\n        avg_rec = total_rec_loss / len(dataloader)\n        avg_vq = total_vq_loss / len(dataloader)\n        \n        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Rec={avg_rec:.4f}, VQ={avg_vq:.4f}\")\n        \n        # Save checkpoint every 10 epochs\n        if (epoch + 1) % 10 == 0:\n            torch.save({\n                'model_state_dict': codec.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'epoch': epoch,\n                'loss': avg_loss\n            }, f\"codec_epoch_{epoch+1}.pth\")\n    \n    # Save final model\n    torch.save(codec.state_dict(), save_path)\n    print(f\"Training completed! Model saved to {save_path}\")\n    \n    return codec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.884363Z","iopub.execute_input":"2025-05-25T17:58:22.884559Z","iopub.status.idle":"2025-05-25T17:58:22.898524Z","shell.execute_reply.started":"2025-05-25T17:58:22.884542Z","shell.execute_reply":"2025-05-25T17:58:22.897735Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ADDED: Testing function\ndef test_codec(model_path: str, test_audio_path: str, output_path: str = \"reconstructed.wav\"):\n    \"\"\"\n    Test the trained codec on a single audio file\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load model\n    codec = AcousticCodec().to(device)\n    codec.load_state_dict(torch.load(model_path, map_location=device))\n    codec.eval()\n    \n    # Load test audio\n    audio, sr = torchaudio.load(test_audio_path)\n    \n    # Resample if needed\n    if sr != codec.sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, codec.sample_rate)\n        audio = resampler(audio)\n    \n    # Convert to mono and add batch dimension\n    if audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True)\n    audio = audio.unsqueeze(0).to(device)  # Add batch dimension\n    \n    # Test encoding/decoding\n    with torch.no_grad():\n        # Full reconstruction\n        reconstructed, tokens, vq_loss = codec(audio)\n        \n        # Just encoding\n        encoded_tokens = codec.encode(audio)\n        \n        # Just decoding  \n        decoded_audio = codec.decode(encoded_tokens)\n        \n        print(f\"Original shape: {audio.shape}\")\n        print(f\"Reconstructed shape: {reconstructed.shape}\")\n        print(f\"Number of token layers: {len(tokens)}\")\n        print(f\"Token shapes: {[t.shape for t in tokens]}\")\n        print(f\"VQ Loss: {vq_loss.item():.4f}\")\n    \n    # Save reconstructed audio\n    torchaudio.save(output_path, reconstructed.squeeze(0).cpu(), codec.sample_rate)\n    print(f\"Reconstructed audio saved to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.899162Z","iopub.execute_input":"2025-05-25T17:58:22.899328Z","iopub.status.idle":"2025-05-25T17:58:22.915539Z","shell.execute_reply.started":"2025-05-25T17:58:22.899315Z","shell.execute_reply":"2025-05-25T17:58:22.914645Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Example usage\nif __name__ == \"__main__\":\n     # For training:\n    codec = train_codec(\n        audio_dir=\"/kaggle/input/tts-dataset/FINAL_TTS_DATA/\",\n        num_epochs=10,\n        batch_size=4,  # Start small\n        learning_rate=1e-4\n    )\n    \n    # For testing:\n    test_codec(\n        model_path=\"acoustic_codec.pth\",\n        test_audio_path=\"/kaggle/input/tts-dataset/FINAL_TTS_DATA/processed_00xcPM_229.mp3\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:58:22.916290Z","iopub.execute_input":"2025-05-25T17:58:22.916530Z","iopub.status.idle":"2025-05-25T19:04:09.523943Z","shell.execute_reply.started":"2025-05-25T17:58:22.916514Z","shell.execute_reply":"2025-05-25T19:04:09.523030Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nFound 8000 audio files\nModel parameters: 1,345,417\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 2000/2000 [06:34<00:00,  5.07it/s, Loss=0.0165, Rec=0.0121, VQ=0.0435]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss=1.2942, Rec=0.0208, VQ=12.7345\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 2000/2000 [06:34<00:00,  5.08it/s, Loss=0.0136, Rec=0.0092, VQ=0.0440]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss=0.0136, Rec=0.0086, VQ=0.0503\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 2000/2000 [06:33<00:00,  5.08it/s, Loss=0.0091, Rec=0.0061, VQ=0.0295]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss=0.0096, Rec=0.0069, VQ=0.0272\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 2000/2000 [06:34<00:00,  5.08it/s, Loss=0.0074, Rec=0.0061, VQ=0.0139]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss=0.0083, Rec=0.0063, VQ=0.0199\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 2000/2000 [06:34<00:00,  5.07it/s, Loss=0.0062, Rec=0.0049, VQ=0.0136]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss=0.0072, Rec=0.0057, VQ=0.0151\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 2000/2000 [06:33<00:00,  5.08it/s, Loss=0.0051, Rec=0.0041, VQ=0.0101]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss=0.0071, Rec=0.0058, VQ=0.0128\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 2000/2000 [06:34<00:00,  5.08it/s, Loss=0.0071, Rec=0.0060, VQ=0.0108]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss=0.0063, Rec=0.0052, VQ=0.0111\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 2000/2000 [06:33<00:00,  5.08it/s, Loss=0.0073, Rec=0.0065, VQ=0.0072]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss=0.0061, Rec=0.0051, VQ=0.0095\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 2000/2000 [06:33<00:00,  5.08it/s, Loss=0.0060, Rec=0.0051, VQ=0.0094]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss=0.0057, Rec=0.0048, VQ=0.0087\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 2000/2000 [06:34<00:00,  5.07it/s, Loss=0.0060, Rec=0.0049, VQ=0.0105]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss=0.0055, Rec=0.0047, VQ=0.0081\nTraining completed! Model saved to acoustic_codec.pth\nOriginal shape: torch.Size([1, 1, 115226])\nReconstructed shape: torch.Size([1, 1, 115232])\nNumber of token layers: 12\nToken shapes: [torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202]), torch.Size([1, 7202])]\nVQ Loss: 0.0062\nReconstructed audio saved to reconstructed.wav\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"    # For testing:\n    test_codec(\n        model_path=\"acoustic_codec.pth\",\n        test_audio_path=\"/kaggle/input/tts-dataset/FINAL_TTS_DATA/processed_01kOfp_182.mp3\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:10:10.799351Z","iopub.execute_input":"2025-05-25T19:10:10.800094Z","iopub.status.idle":"2025-05-25T19:10:10.961145Z","shell.execute_reply.started":"2025-05-25T19:10:10.800062Z","shell.execute_reply":"2025-05-25T19:10:10.960488Z"}},"outputs":[{"name":"stdout","text":"Original shape: torch.Size([1, 1, 132456])\nReconstructed shape: torch.Size([1, 1, 132464])\nNumber of token layers: 12\nToken shapes: [torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279]), torch.Size([1, 8279])]\nVQ Loss: 0.0048\nReconstructed audio saved to reconstructed.wav\n","output_type":"stream"}],"execution_count":13}]}