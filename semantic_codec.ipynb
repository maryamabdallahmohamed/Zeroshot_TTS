{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2db31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_codec=raw_audio -> s-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5631946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryamsaad/Documents/Zeroshot_TTS/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/maryamsaad/Documents/Zeroshot_TTS/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModel, WhisperProcessor, WhisperForConditionalGeneration\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import gc\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe815fe",
   "metadata": {},
   "source": [
    "### Loading Data and Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb921e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio files: 45000\n",
      "Processing Audio Files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio Files: 100%|██████████| 2813/2813 [2:53:59<00:00,  3.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete. Features saved to: features_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename='training.log', filemode='w')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the dataset class\n",
    "class Arabic_Processed_audios(Dataset):\n",
    "    def __init__(self, audio_path, max_length=160000):\n",
    "        self.audio_path = audio_path\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_path, \"*.mp3\"))\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.audio_files[idx]\n",
    "        try:\n",
    "            audio, _ = librosa.load(file, sr=16000)\n",
    "            audio = librosa.util.normalize(audio)\n",
    "            \n",
    "            if len(audio) < self.max_length:\n",
    "                audio = np.pad(audio, (0, self.max_length - len(audio)))\n",
    "            else:\n",
    "                audio = audio[:self.max_length]\n",
    "            \n",
    "            return audio, os.path.basename(file)  # Return filename for saving features\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {file}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "# Load audio files\n",
    "Audio_files = Arabic_Processed_audios('phase2_data/subset_80k_audio')\n",
    "print(f\"Number of audio files: {len(Audio_files)}\")\n",
    "\n",
    "# Load Whisper processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", cache_dir='./processor_cache')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\", cache_dir='./model_cache')\n",
    "device = torch.device(\"cpu\")  # Explicitly set to CPU since MPS is unsupported\n",
    "model.to(device)\n",
    "\n",
    "# Enable mixed precision if possible\n",
    "\n",
    "def extract_whisper_features(model, audio, processor, layer=-1, max_length=1500):\n",
    "    try:\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = model.get_encoder()(inputs[\"input_features\"])\n",
    "            features = encoder_outputs.last_hidden_state if layer == -1 else encoder_outputs.hidden_states[layer]\n",
    "        # Pad or truncate to max_length\n",
    "        if features.shape[1] > max_length:\n",
    "            features = features[:, :max_length, :]\n",
    "        elif features.shape[1] < max_length:\n",
    "            padding = (0, 0, 0, max_length - features.shape[1])\n",
    "            features = torch.nn.functional.pad(features, padding)\n",
    "        return features.cpu()  # Move to CPU to save memory\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing audio: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create output directory for features\n",
    "output_dir = \"features_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Use DataLoader for batch processing\n",
    "dataloader = DataLoader(Audio_files, batch_size=16, shuffle=False, num_workers=0)  # Adjust batch_size as needed\n",
    "\n",
    "print(\"Processing Audio Files\")\n",
    "for batch in tqdm(dataloader, desc=\"Processing Audio Files\"):\n",
    "    audios, filenames = batch\n",
    "    for audio, filename in zip(audios, filenames):\n",
    "        if audio is None:\n",
    "            continue  # Skip problematic files\n",
    "        features = extract_whisper_features(model, audio.numpy(), processor, layer=-1)\n",
    "        if features is not None:\n",
    "            # Save features to disk\n",
    "            feature_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_features.npy\")\n",
    "            np.save(feature_path, features.numpy())\n",
    "        \n",
    "        # Clear memory\n",
    "        del features, audio\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"Feature extraction complete. Features saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791223f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, features_dir):\n",
    "        self.features_dir = features_dir\n",
    "        self.feature_files = glob.glob(os.path.join(features_dir, \"*_features.npy\"))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        feature_file = self.feature_files[idx]\n",
    "        features = np.load(feature_file)\n",
    "        return torch.FloatTensor(features)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a47932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature files: 45000\n"
     ]
    }
   ],
   "source": [
    "# Load features dataset\n",
    "features_dataset = FeaturesDataset('features_output')\n",
    "print(f\"Number of feature files: {len(features_dataset)}\")\n",
    "\n",
    "# Create train/test split\n",
    "train_size = int(0.8 * len(features_dataset))\n",
    "test_size = len(features_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(features_dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06019758",
   "metadata": {},
   "source": [
    "##### Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dba09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size, padding=kernel_size//2, groups=dim)  # Depthwise\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, dim, seq_len)\n",
    "        residual = x\n",
    "        x = self.conv(x)\n",
    "        # Transpose for LayerNorm\n",
    "        x = x.transpose(1, 2)  # (batch, seq_len, dim)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        # Transpose back\n",
    "        x = x.transpose(1, 2)  # (batch, dim, seq_len)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb5e87",
   "metadata": {},
   "source": [
    "##### Vector Quantization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e749620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Quantization Layer\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings=8192, embedding_dim=8, commitment_cost=0.25):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.embeddings = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
    "        self.register_buffer('ema_count', torch.zeros(num_embeddings))\n",
    "        self.register_buffer('ema_weight', self.embeddings.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        flat_x = x.reshape(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flat_x, self.embeddings)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embeddings[encoding_indices].reshape(x.shape)\n",
    "        codebook_loss = F.mse_loss(quantized.detach(), x)\n",
    "        commitment_loss = self.commitment_cost * F.mse_loss(quantized, x.detach())\n",
    "        loss = codebook_loss + commitment_loss\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                one_hot = F.one_hot(encoding_indices, self.num_embeddings).float()\n",
    "                self.ema_count = 0.999 * self.ema_count + 0.001 * torch.sum(one_hot, dim=0)\n",
    "                n = torch.sum(self.ema_count)\n",
    "                self.ema_count = (self.ema_count + 1e-8) / (n + self.num_embeddings * 1e-8) * n\n",
    "                dw = torch.matmul(one_hot.transpose(0, 1), flat_x)\n",
    "                self.ema_weight = 0.999 * self.ema_weight + 0.001 * dw\n",
    "                self.embeddings.data = (self.ema_weight / (self.ema_count.unsqueeze(-1) + 1e-8))\n",
    "        \n",
    "        return quantized, loss, encoding_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfc95b",
   "metadata": {},
   "source": [
    "#### Semantic Codec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612d783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE Model\n",
    "# Custom Lambda module for applying arbitrary functions (e.g., transpose)\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=384, codebook_size=8192, codebook_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "                    nn.Conv1d(input_dim, hidden_dim, kernel_size=7, padding=3),\n",
    "                    Lambda(lambda x: x.transpose(1, 2)),  # To (batch_size, sequence_length, hidden_dim)\n",
    "                    nn.LayerNorm(hidden_dim),\n",
    "                    Lambda(lambda x: x.transpose(1, 2)),  # Back to (batch_size, hidden_dim, sequence_length)\n",
    "                    # Assume ConvNextBlock is defined and works with (batch_size, hidden_dim, sequence_length)\n",
    "                    *[ConvNextBlock(hidden_dim) for _ in range(6)],\n",
    "                    nn.Conv1d(hidden_dim, codebook_dim, kernel_size=1)\n",
    "                )\n",
    "        self.quantizer = VectorQuantizer(num_embeddings=codebook_size, embedding_dim=codebook_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(codebook_dim, hidden_dim, kernel_size=7, padding=3),\n",
    "            *[ConvNextBlock(hidden_dim) for _ in range(6)],\n",
    "            nn.Conv1d(hidden_dim, input_dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = x.transpose(1, 2)  # From (batch_size, sequence_length, input_dim) to (batch_size, input_dim, sequence_length)\n",
    "            z = self.encoder(x)  # Output: (batch_size, codebook_dim, sequence_length)\n",
    "            z = z.transpose(1, 2)  # To (batch_size, sequence_length, codebook_dim) for quantizer\n",
    "            quantized, vq_loss, indices = self.quantizer(z)  # quantized: (batch_size, sequence_length, codebook_dim)\n",
    "            quantized = quantized.transpose(1, 2)  # To (batch_size, codebook_dim, sequence_length)\n",
    "            recon = self.decoder(quantized)  # Input to decoder: (batch_size, codebook_dim, sequence_length)\n",
    "            return recon, quantized, vq_loss, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb96418",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191f62a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature files: 45000\n"
     ]
    }
   ],
   "source": [
    "# Load features dataset\n",
    "features_dataset = FeaturesDataset('features_output')\n",
    "print(f\"Number of feature files: {len(features_dataset)}\")\n",
    "\n",
    "# Create train/test split\n",
    "train_size = int(0.8 * len(features_dataset))\n",
    "test_size = len(features_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(features_dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb255f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VQVAE model\n",
    "device='mps'\n",
    "model = VQVAE(input_dim=512, hidden_dim=384, codebook_size=8192, codebook_dim=8)\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da44f6",
   "metadata": {},
   "source": [
    "#### Training and evaluation of semantic codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b930e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, test_dataloader, num_epochs=10):   \n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_vq_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
    "            features = batch[0].to(device)\n",
    "            recon, quantized, vq_loss, indices = model(features)\n",
    "            recon = recon.transpose(1, 2)  # Back to (batch_size, 1500, 512)\n",
    "            # Normalize the loss by batch size\n",
    "            recon_loss = torch.nn.functional.mse_loss(recon, batch[0].to(device), reduction='mean')\n",
    "            loss = recon_loss + vq_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track individual losses\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_vq_loss += vq_loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Print epoch statistics once per epoch\n",
    "        avg_recon_loss = epoch_recon_loss / len(train_dataloader)\n",
    "        avg_vq_loss = epoch_vq_loss / len(train_dataloader)\n",
    "        avg_total_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Training - Recon Loss: {avg_recon_loss:.4f}, VQ Loss: {avg_vq_loss:.4f}, Total Loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_recon_loss = 0\n",
    "            test_vq_loss = 0\n",
    "            test_total_loss = 0\n",
    "            \n",
    "            for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "                features = batch[0].to(device)\n",
    "                recon, quantized, vq_loss, indices = model(features)\n",
    "                recon = recon.transpose(1, 2)\n",
    "                # Use the same loss calculation as training\n",
    "                recon_loss = torch.nn.functional.mse_loss(recon, batch[0].to(device), reduction='mean')\n",
    "                total_loss = recon_loss + vq_loss\n",
    "                \n",
    "                test_recon_loss += recon_loss.item()\n",
    "                test_vq_loss += vq_loss.item()\n",
    "                test_total_loss += total_loss.item()\n",
    "            \n",
    "            # Print test statistics\n",
    "            avg_test_recon_loss = test_recon_loss / len(test_dataloader)\n",
    "            avg_test_vq_loss = test_vq_loss / len(test_dataloader)\n",
    "            avg_test_total_loss = test_total_loss / len(test_dataloader)\n",
    "            print(f\"  Testing - Recon Loss: {avg_test_recon_loss:.4f}, VQ Loss: {avg_test_vq_loss:.4f}, Total Loss: {avg_test_total_loss:.4f}\\n\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1616635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "  Training - Recon Loss: 2.5877, VQ Loss: 0.4990, Total Loss: 3.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:22<00:00, 24.80it/s]\n",
      "Epochs:   7%|▋         | 1/15 [03:26<48:15, 206.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.4096, VQ Loss: 0.2361, Total Loss: 1.6457\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/15\n",
      "  Training - Recon Loss: 2.6228, VQ Loss: 0.2166, Total Loss: 2.8394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:22<00:00, 24.69it/s]\n",
      "Epochs:  13%|█▎        | 2/15 [06:41<43:15, 199.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.8586, VQ Loss: 0.1596, Total Loss: 2.0182\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/15\n",
      "  Training - Recon Loss: 2.1301, VQ Loss: 0.1619, Total Loss: 2.2920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 24.01it/s]\n",
      "Epochs:  20%|██        | 3/15 [09:58<39:41, 198.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.4287, VQ Loss: 0.1768, Total Loss: 1.6055\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/15\n",
      "  Training - Recon Loss: 1.7337, VQ Loss: 0.1154, Total Loss: 1.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 24.07it/s]\n",
      "Epochs:  27%|██▋       | 4/15 [13:16<36:20, 198.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.7483, VQ Loss: 0.1132, Total Loss: 1.8615\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/15\n",
      "  Training - Recon Loss: 1.4932, VQ Loss: 0.0950, Total Loss: 1.5883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 24.38it/s]\n",
      "Epochs:  33%|███▎      | 5/15 [16:33<32:56, 197.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.3937, VQ Loss: 0.0913, Total Loss: 1.4850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/15\n",
      "  Training - Recon Loss: 1.3745, VQ Loss: 0.1139, Total Loss: 1.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:22<00:00, 24.62it/s]\n",
      "Epochs:  40%|████      | 6/15 [19:49<29:34, 197.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.3656, VQ Loss: 0.0275, Total Loss: 1.3931\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/15\n",
      "  Training - Recon Loss: 99.6538, VQ Loss: 434.5145, Total Loss: 534.1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 24.09it/s]\n",
      "Epochs:  47%|████▋     | 7/15 [23:04<26:13, 196.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.4037, VQ Loss: 0.9941, Total Loss: 2.3978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/15\n",
      "  Training - Recon Loss: 1.3277, VQ Loss: 0.6148, Total Loss: 1.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 24.04it/s]\n",
      "Epochs:  53%|█████▎    | 8/15 [26:22<22:58, 196.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.2825, VQ Loss: 0.3829, Total Loss: 1.6653\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/15\n",
      "  Training - Recon Loss: 1.7769, VQ Loss: 0.3563, Total Loss: 2.1332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 23.81it/s]\n",
      "Epochs:  60%|██████    | 9/15 [29:40<19:43, 197.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.1900, VQ Loss: 0.2856, Total Loss: 1.4756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/15\n",
      "  Training - Recon Loss: 2.0055, VQ Loss: 0.2589, Total Loss: 2.2645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 23.77it/s]\n",
      "Epochs:  67%|██████▋   | 10/15 [32:59<16:28, 197.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.1931, VQ Loss: 0.1805, Total Loss: 1.3736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/15\n",
      "  Training - Recon Loss: 1.5074, VQ Loss: 0.2493, Total Loss: 1.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 23.80it/s]\n",
      "Epochs:  73%|███████▎  | 11/15 [36:17<13:11, 197.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.2073, VQ Loss: 0.1684, Total Loss: 1.3757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/15\n",
      "  Training - Recon Loss: 418.8888, VQ Loss: 23665.5423, Total Loss: 24084.4307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 24.16it/s]\n",
      "Epochs:  80%|████████  | 12/15 [39:33<09:52, 197.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.4731, VQ Loss: 32.0831, Total Loss: 33.5562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/15\n",
      "  Training - Recon Loss: 1.3003, VQ Loss: 5.3324, Total Loss: 6.6327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 23.69it/s]\n",
      "Epochs:  87%|████████▋ | 13/15 [42:51<06:35, 197.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.2372, VQ Loss: 2.3025, Total Loss: 3.5397\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/15\n",
      "  Training - Recon Loss: 1.8435, VQ Loss: 1.5969, Total Loss: 3.4404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 23.86it/s]\n",
      "Epochs:  93%|█████████▎| 14/15 [46:10<03:17, 197.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.2850, VQ Loss: 0.9127, Total Loss: 2.1977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/15\n",
      "  Training - Recon Loss: 3.1348, VQ Loss: 0.7053, Total Loss: 3.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 563/563 [00:23<00:00, 23.82it/s]\n",
      "Epochs: 100%|██████████| 15/15 [49:28<00:00, 197.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 18.3410, VQ Loss: 0.7024, Total Loss: 19.0434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "model = train_model(model, train_dataloader, test_dataloader, num_epochs=15)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'vqvae_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e27d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 563/563 [00:23<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "  Reconstruction Loss: 18.3410\n",
      "  VQ Loss: 0.7024\n",
      "  Total Loss: 19.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2250/2250 [01:33<00:00, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "  Reconstruction Loss: 18.3371\n",
      "  VQ Loss: 0.7100\n",
      "  Total Loss: 19.0471\n",
      "Training Evaluation:\n",
      "  Reconstruction Loss: 18.3371\n",
      "  VQ Loss: 0.7100\n",
      "  Total Loss: 19.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a validation dataset (val_dataset) and corresponding dataloader (val_dataloader)\n",
    "# If not, you'll need to create one.  This example assumes you have one.\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_recon_loss = 0.0\n",
    "    total_vq_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            features = batch[0].to(device)  # Assuming your data loader returns a list, and the first element is your feature tensor\n",
    "            \n",
    "            recon, quantized, vq_loss, indices = model(features)\n",
    "            recon = recon.transpose(1, 2)\n",
    "            recon_loss = torch.nn.functional.mse_loss(recon, features, reduction='mean')\n",
    "            loss = recon_loss + vq_loss\n",
    "\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_vq_loss += vq_loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_recon_loss = total_recon_loss / num_batches\n",
    "    avg_vq_loss = total_vq_loss / num_batches\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"  Reconstruction Loss: {avg_recon_loss:.4f}\")\n",
    "    print(f\"  VQ Loss: {avg_vq_loss:.4f}\")\n",
    "    print(f\"  Total Loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "    return avg_recon_loss, avg_vq_loss, avg_total_loss\n",
    "\n",
    "# Example usage (assuming you have a validation dataloader named 'val_dataloader')\n",
    "# Make sure your model is on the correct device before evaluating\n",
    "device = 'mps'\n",
    "model = model.to(device)  # Ensure model is on the correct device\n",
    "\n",
    "recon_loss, vq_loss, total_loss = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "# You can also evaluate on the training data to check for overfitting:\n",
    "train_recon_loss, train_vq_loss, train_total_loss = evaluate_model(model, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "011ecc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'semantic_codec_final.pth') #saved with state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da493ef0",
   "metadata": {},
   "source": [
    "### T2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fadb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskGCTT2S(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        semantic_vocab_size=8192,\n",
    "        n_layers=16,\n",
    "        d_model=1024,\n",
    "        d_ff=4096,\n",
    "        n_heads=16,\n",
    "        max_seq_len=2048,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Text embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Semantic token embeddings (for masked tokens)\n",
    "        self.semantic_embeddings = nn.Embedding(semantic_vocab_size, d_model)\n",
    "        \n",
    "        # Position embeddings (RoPE)\n",
    "        self.pos_encoding = RotaryPositionalEmbedding(d_model // n_heads, max_seq_len)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(\n",
    "                d_model=d_model, \n",
    "                d_ff=d_ff, \n",
    "                n_heads=n_heads,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True,  # Important: use bidirectional attention\n",
    "                with_rope=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, semantic_vocab_size)\n",
    "        \n",
    "        # Adaptive RMSNorm\n",
    "        self.norm = AdaptiveRMSNorm(d_model)\n",
    "        \n",
    "    def forward(self, text_tokens, semantic_tokens=None, masks=None, timestep=None):\n",
    "        \"\"\"\n",
    "        Forward pass with masked semantic tokens\n",
    "        Args:\n",
    "            text_tokens: Input text tokens [B, T_text]\n",
    "            semantic_tokens: Target semantic tokens [B, T_semantic]\n",
    "            masks: Masking tensor for semantic tokens [B, T_semantic]\n",
    "            timestep: Current diffusion timestep\n",
    "        \"\"\"\n",
    "        # Get batch size\n",
    "        batch_size = text_tokens.size(0)\n",
    "        \n",
    "        # Text embeddings\n",
    "        text_emb = self.token_embeddings(text_tokens)\n",
    "        \n",
    "        if semantic_tokens is not None:\n",
    "            # Semantic embeddings\n",
    "            semantic_emb = self.semantic_embeddings(semantic_tokens)\n",
    "            \n",
    "            # Apply masking\n",
    "            if masks is not None:\n",
    "                # Replace masked tokens with learned mask embedding\n",
    "                semantic_emb = semantic_emb * (1 - masks.unsqueeze(-1))\n",
    "            \n",
    "            # Concatenate text and semantic embeddings\n",
    "            x = torch.cat([text_emb, semantic_emb], dim=1)\n",
    "        else:\n",
    "            x = text_emb\n",
    "            \n",
    "        # Process through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, timestep=timestep)\n",
    "            \n",
    "        # Normalize output\n",
    "        x = self.norm(x, timestep)\n",
    "        \n",
    "        # Get only the semantic part predictions\n",
    "        if semantic_tokens is not None:\n",
    "            semantic_len = semantic_tokens.size(1)\n",
    "            semantic_preds = x[:, -semantic_len:]\n",
    "        else:\n",
    "            semantic_preds = x[:, text_tokens.size(1):]\n",
    "            \n",
    "        # Project to semantic vocabulary\n",
    "        logits = self.output_proj(semantic_preds)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DurationPredictor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        n_layers=12,\n",
    "        d_model=768,\n",
    "        n_heads=12,\n",
    "        max_seq_len=2048,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Similar architecture to main model but smaller\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = RotaryPositionalEmbedding(d_model // n_heads, max_seq_len)\n",
    "        \n",
    "        # Transformer layers with bidirectional attention\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(\n",
    "                d_model=d_model, \n",
    "                d_ff=d_model*4, \n",
    "                n_heads=n_heads,\n",
    "                bidirectional=True,\n",
    "                with_rope=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection for duration prediction\n",
    "        self.output_proj = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, phoneme_tokens, prompt_phonemes=None, prompt_durations=None, timestep=None):\n",
    "        \"\"\"Flow matching based duration prediction\"\"\"\n",
    "        # Implementation of flow matching for duration prediction\n",
    "        # See Section A.5 in the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_t2s_model(model, data_loader, optimizer, device, mask_ratio=0.75):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        # Unpack batch\n",
    "        text_tokens = batch[\"text_tokens\"].to(device)\n",
    "        semantic_tokens = batch[\"semantic_tokens\"].to(device)\n",
    "        \n",
    "        # Create random masks for semantic tokens\n",
    "        masks = torch.bernoulli(\n",
    "            torch.ones_like(semantic_tokens) * mask_ratio\n",
    "        ).to(device)\n",
    "        \n",
    "        # Random timestep for adaptive layer norm\n",
    "        timestep = torch.rand(text_tokens.size(0), device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(text_tokens, semantic_tokens, masks, timestep)\n",
    "        \n",
    "        # Compute loss only on masked tokens\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            semantic_tokens.view(-1),\n",
    "            reduction=\"none\"\n",
    "        )\n",
    "        \n",
    "        # Apply masking to loss\n",
    "        loss = (loss * masks.view(-1)).sum() / (masks.sum() + 1e-8)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb539d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_tokens(\n",
    "    model, \n",
    "    text_tokens, \n",
    "    duration_predictor,\n",
    "    num_steps=25,  # Paper shows 25 steps is optimal\n",
    "    device=\"cuda\",\n",
    "    guidance_scale=2.5,\n",
    "    rescale_weight=0.75,\n",
    "):\n",
    "    model.eval()\n",
    "    batch_size = text_tokens.size(0)\n",
    "    \n",
    "    # Get text length\n",
    "    text_len = text_tokens.size(1)\n",
    "    \n",
    "    # Predict duration using duration predictor\n",
    "    durations = duration_predictor(text_tokens)\n",
    "    total_duration = durations.sum().int().item()\n",
    "    \n",
    "    # Initialize semantic tokens randomly\n",
    "    semantic_tokens = torch.randint(\n",
    "        0, model.semantic_vocab_size, (batch_size, total_duration),\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Initialize with all tokens masked\n",
    "    masks = torch.ones_like(semantic_tokens, device=device)\n",
    "    \n",
    "    # Progressive generation with 25 inference steps\n",
    "    for step in range(num_steps):\n",
    "        # Forward pass with current state\n",
    "        with torch.no_grad():\n",
    "            # Get conditional output\n",
    "            logits_cond = model(text_tokens, semantic_tokens, masks)\n",
    "            \n",
    "            # Classifier-free guidance: Get unconditional output\n",
    "            if guidance_scale > 1.0:\n",
    "                # Drop prompt with probability 0.15 during training\n",
    "                logits_uncond = model(torch.zeros_like(text_tokens), semantic_tokens, masks)\n",
    "                \n",
    "                # Apply classifier-free guidance\n",
    "                logits = logits_cond + guidance_scale * (logits_cond - logits_uncond)\n",
    "                \n",
    "                # Apply rescaling\n",
    "                std_cond = torch.std(logits_cond, dim=-1, keepdim=True)\n",
    "                std_guided = torch.std(logits, dim=-1, keepdim=True)\n",
    "                logits_rescaled = logits * (std_cond / std_guided)\n",
    "                \n",
    "                # Final logits\n",
    "                logits = rescale_weight * logits_rescaled + (1 - rescale_weight) * logits\n",
    "            else:\n",
    "                logits = logits_cond\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample new tokens\n",
    "        new_tokens = torch.multinomial(probs.view(-1, probs.size(-1)), 1).view(batch_size, -1)\n",
    "        \n",
    "        # Update semantic tokens at masked positions\n",
    "        semantic_tokens = semantic_tokens * (1 - masks) + new_tokens * masks\n",
    "        \n",
    "        # Update masks for next iteration (progressively unmask)\n",
    "        if step < num_steps - 1:\n",
    "            # Create new random masks for remaining steps\n",
    "            unmasked_ratio = (step + 1) / num_steps\n",
    "            masks = torch.bernoulli(\n",
    "                torch.ones_like(semantic_tokens) * (1 - unmasked_ratio)\n",
    "            ).to(device)\n",
    "    \n",
    "    return semantic_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(generated_speech, reference_speech, asr_model):\n",
    "    \"\"\"Calculate SIM and WER metrics\"\"\"\n",
    "    # SIM: Speaker similarity using embeddings\n",
    "    sim_score = calculate_similarity(generated_speech, reference_speech)\n",
    "    \n",
    "    # WER: Word Error Rate using ASR\n",
    "    generated_text = asr_model.transcribe(generated_speech)\n",
    "    reference_text = asr_model.transcribe(reference_speech)\n",
    "    wer_score = calculate_wer(generated_text, reference_text)\n",
    "    \n",
    "    return sim_score, wer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8892c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = create_tokenizer()  # G2P for English, BPE+jieba+pypinyin for Chinese\n",
    "semantic_codec = SemanticCodec().to(device)\n",
    "t2s_model = MaskGCTT2S(...).to(device)\n",
    "duration_predictor = DurationPredictor(...).to(device)\n",
    "\n",
    "# Tokenize input text\n",
    "text = \"Hello, this is a test.\"\n",
    "text_tokens = text_tokenizer.tokenize(text)\n",
    "\n",
    "# Generate semantic tokens\n",
    "semantic_tokens = generate_semantic_tokens(\n",
    "    t2s_model, \n",
    "    text_tokens, \n",
    "    duration_predictor,\n",
    "    num_steps=25  # As recommended in paper\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
