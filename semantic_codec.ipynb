{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2db31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_codec=raw_audio -> s-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5631946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryamsaad/Documents/Zeroshot_TTS/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/maryamsaad/Documents/Zeroshot_TTS/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset,  random_split\n",
    "import numpy as np\n",
    "from transformers import  WhisperProcessor, WhisperForConditionalGeneration\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import gc\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81758e9",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe815fe",
   "metadata": {},
   "source": [
    "### Loading Data and Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb921e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio files: 20000\n",
      "Processing Audio Files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio Files: 100%|██████████| 1250/1250 [1:12:57<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename='training.log', filemode='w')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the dataset class\n",
    "class Arabic_Processed_audios(Dataset):\n",
    "    def __init__(self, audio_path, max_length=160000):\n",
    "        self.audio_path = audio_path\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_path, \"*.mp3\"))[:20000]\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.audio_files[idx]\n",
    "        try:\n",
    "            audio, _ = librosa.load(file, sr=16000)\n",
    "            audio = librosa.util.normalize(audio)\n",
    "            \n",
    "            if len(audio) < self.max_length:\n",
    "                audio = np.pad(audio, (0, self.max_length - len(audio)))\n",
    "            else:\n",
    "                audio = audio[:self.max_length]\n",
    "            \n",
    "            return audio, os.path.basename(file)  # Return filename for saving features\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {file}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "# Load audio files\n",
    "Audio_files = Arabic_Processed_audios('phase2_data/subset_80k_audio')\n",
    "print(f\"Number of audio files: {len(Audio_files)}\")\n",
    "\n",
    "# Load Whisper processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", cache_dir='./processor_cache')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\", cache_dir='./model_cache')\n",
    "device = torch.device(\"cpu\")  # Explicitly set to CPU since MPS is unsupported\n",
    "model.to(device)\n",
    "\n",
    "def extract_whisper_features(model, audio, processor, layer=-1, max_length=1500):\n",
    "    try:\n",
    "        # For 10-second audio at 16kHz, we need at least 160,000 samples\n",
    "        min_samples = 160000  # 10 seconds * 16000 Hz\n",
    "        \n",
    "        # Pad audio if it's shorter than 10 seconds\n",
    "        if len(audio) < min_samples:\n",
    "            audio = np.pad(audio, (0, min_samples - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:min_samples]  # Truncate if longer\n",
    "            \n",
    "        # Process with Whisper\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = model.get_encoder()(inputs[\"input_features\"])\n",
    "            features = encoder_outputs.last_hidden_state if layer == -1 else encoder_outputs.hidden_states[layer]\n",
    "            \n",
    "        # Pad or truncate features to max_length\n",
    "        if features.shape[1] > max_length:\n",
    "            features = features[:, :max_length, :]\n",
    "        elif features.shape[1] < max_length:\n",
    "            padding = (0, 0, 0, max_length - features.shape[1])\n",
    "            features = torch.nn.functional.pad(features, padding)\n",
    "            \n",
    "        return features.cpu()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing audio: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Use DataLoader for batch processing\n",
    "dataloader = DataLoader(Audio_files, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Processing Audio Files\")\n",
    "for batch in tqdm(dataloader, desc=\"Processing Audio Files\"):\n",
    "    audios, filenames = batch\n",
    "    for audio, filename in zip(audios, filenames):\n",
    "        if audio is None:\n",
    "            continue  \n",
    "        features = extract_whisper_features(model, audio.numpy(), processor, layer=-1)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66193112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06b1a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 145 feature files...\n",
      "Loaded features shape: (145, 1, 1500, 512)\n",
      "Number of features in dataset: 145\n",
      "Train batch shape: torch.Size([16, 1500, 512])\n",
      "Test batch shape: torch.Size([16, 1500, 512])\n",
      "\n",
      "Number of training samples: 116\n",
      "Number of test samples: 29\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# First, load all features from the files\n",
    "features_dir = 'features_output'\n",
    "feature_files = glob.glob(os.path.join(features_dir, \"*_features.npy\"))\n",
    "features = []\n",
    "\n",
    "print(f\"Loading {len(feature_files)} feature files...\")\n",
    "for feature_file in feature_files:\n",
    "    feature = np.load(feature_file)  # Shape: (1, 1500, 512)\n",
    "    features.append(feature)\n",
    "\n",
    "# Convert list to numpy array\n",
    "features = np.array(features)  # Shape: (N, 1, 1500, 512) where N is number of files\n",
    "print(f\"Loaded features shape: {features.shape}\")\n",
    "\n",
    "class WhisperFeaturesDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        feature = feature.squeeze(0)  # Shape: (1500, 512)\n",
    "        return torch.FloatTensor(feature)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "# Create dataset\n",
    "features_dataset = WhisperFeaturesDataset(features)\n",
    "print(f\"Number of features in dataset: {len(features_dataset)}\")\n",
    "\n",
    "# Create train/test split (80% train, 20% test)\n",
    "train_size = int(0.8 * len(features_dataset))\n",
    "test_size = len(features_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(features_dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 to avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Verify the shapes\n",
    "for batch in train_dataloader:\n",
    "    print(\"Train batch shape:\", batch.shape)  # Should be (16, 1500, 512)\n",
    "    break\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    print(\"Test batch shape:\", batch.shape)  # Should be (16, 1500, 512)\n",
    "    break\n",
    "\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e6576",
   "metadata": {},
   "source": [
    "## Semantic Codec Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff2030bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature files: 145\n"
     ]
    }
   ],
   "source": [
    "# class FeaturesDataset(Dataset):\n",
    "#     def __init__(self, features_dir):\n",
    "#         self.features_dir = features_dir\n",
    "#         self.feature_files = glob.glob(os.path.join(features_dir, \"*_features.npy\"))\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         feature_file = self.feature_files[idx]\n",
    "#         features = np.load(feature_file)  # Shape: (1, 1500, 512)\n",
    "#         # Remove the batch dimension since DataLoader will handle batching\n",
    "#         features = features.squeeze(0)  # Shape: (1500, 512)\n",
    "#         return torch.FloatTensor(features)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.feature_files)\n",
    "\n",
    "# # Create dataset\n",
    "# features_dataset = FeaturesDataset('features_output')\n",
    "# print(f\"Number of feature files: {len(features_dataset)}\")\n",
    "\n",
    "# # Create train/test split\n",
    "# train_size = int(0.8 * len(features_dataset))\n",
    "# test_size = len(features_dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(features_dataset, [train_size, test_size])\n",
    "\n",
    "# # Create dataloaders\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=16, \n",
    "#     shuffle=True,\n",
    "#     num_workers=0  \n",
    "# )\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset, \n",
    "#     batch_size=16, \n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "678c50ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([16, 1500, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for batch in train_dataloader:\n",
    "    print(\"Batch shape:\", batch.shape)  # Should be (16, 1500, 512)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e457fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06019758",
   "metadata": {},
   "source": [
    "##### Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dba09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size, padding=kernel_size//2, groups=dim)  # Depthwise\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, dim, seq_len)\n",
    "        residual = x\n",
    "        x = self.conv(x)\n",
    "        # Transpose for LayerNorm\n",
    "        x = x.transpose(1, 2)  # (batch, seq_len, dim)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        # Transpose back\n",
    "        x = x.transpose(1, 2)  # (batch, dim, seq_len)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb5e87",
   "metadata": {},
   "source": [
    "##### Vector Quantization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e749620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Quantization Layer\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings=8192, embedding_dim=8, commitment_cost=0.25):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.embeddings = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
    "        self.register_buffer('ema_count', torch.zeros(num_embeddings))\n",
    "        self.register_buffer('ema_weight', self.embeddings.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        flat_x = x.reshape(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flat_x, self.embeddings)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embeddings[encoding_indices].reshape(x.shape)\n",
    "        codebook_loss = F.mse_loss(quantized.detach(), x)\n",
    "        commitment_loss = self.commitment_cost * F.mse_loss(quantized, x.detach())\n",
    "        loss = codebook_loss + commitment_loss\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                one_hot = F.one_hot(encoding_indices, self.num_embeddings).float()\n",
    "                self.ema_count = 0.999 * self.ema_count + 0.001 * torch.sum(one_hot, dim=0)\n",
    "                n = torch.sum(self.ema_count)\n",
    "                self.ema_count = (self.ema_count + 1e-8) / (n + self.num_embeddings * 1e-8) * n\n",
    "                dw = torch.matmul(one_hot.transpose(0, 1), flat_x)\n",
    "                self.ema_weight = 0.999 * self.ema_weight + 0.001 * dw\n",
    "                self.embeddings.data = (self.ema_weight / (self.ema_count.unsqueeze(-1) + 1e-8))\n",
    "        \n",
    "        return quantized, loss, encoding_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfc95b",
   "metadata": {},
   "source": [
    "#### Semantic Codec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "612d783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE Model\n",
    "# Custom Lambda module for applying arbitrary functions (e.g., transpose)\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=384, codebook_size=8192, codebook_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "                    nn.Conv1d(input_dim, hidden_dim, kernel_size=7, padding=3),\n",
    "                    Lambda(lambda x: x.transpose(1, 2)),  # To (batch_size, sequence_length, hidden_dim)\n",
    "                    nn.LayerNorm(hidden_dim),\n",
    "                    Lambda(lambda x: x.transpose(1, 2)),  # Back to (batch_size, hidden_dim, sequence_length)\n",
    "                    # Assume ConvNextBlock is defined and works with (batch_size, hidden_dim, sequence_length)\n",
    "                    *[ConvNextBlock(hidden_dim) for _ in range(6)],\n",
    "                    nn.Conv1d(hidden_dim, codebook_dim, kernel_size=1)\n",
    "                )\n",
    "        self.quantizer = VectorQuantizer(num_embeddings=codebook_size, embedding_dim=codebook_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(codebook_dim, hidden_dim, kernel_size=7, padding=3),\n",
    "            *[ConvNextBlock(hidden_dim) for _ in range(6)],\n",
    "            nn.Conv1d(hidden_dim, input_dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = x.transpose(1, 2)  # From (batch_size, sequence_length, input_dim) to (batch_size, input_dim, sequence_length)\n",
    "            z = self.encoder(x)  # Output: (batch_size, codebook_dim, sequence_length)\n",
    "            z = z.transpose(1, 2)  # To (batch_size, sequence_length, codebook_dim) for quantizer\n",
    "            quantized, vq_loss, indices = self.quantizer(z)  # quantized: (batch_size, sequence_length, codebook_dim)\n",
    "            quantized = quantized.transpose(1, 2)  # To (batch_size, codebook_dim, sequence_length)\n",
    "            recon = self.decoder(quantized)  # Input to decoder: (batch_size, codebook_dim, sequence_length)\n",
    "            return recon, quantized, vq_loss, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb96418",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "191f62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load features dataset\n",
    "# features_dataset = FeaturesDataset('features_output')\n",
    "# print(f\"Number of feature files: {len(features_dataset)}\")\n",
    "\n",
    "# # Create train/test split\n",
    "# train_size = int(0.8 * len(features_dataset))\n",
    "# test_size = len(features_dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(features_dataset, [train_size, test_size])\n",
    "\n",
    "# # Create dataloaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb255f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VQVAE model\n",
    "device='mps'\n",
    "model = VQVAE(input_dim=512, hidden_dim=384, codebook_size=8192, codebook_dim=8)\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da44f6",
   "metadata": {},
   "source": [
    "#### Training and evaluation of semantic codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b930e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, test_dataloader, num_epochs):   \n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_vq_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
    "            features = batch.to(device)  # Shape: (batch_size, sequence_length, input_dim)\n",
    "            \n",
    "            # Ensure features are in the correct shape\n",
    "            if len(features.shape) == 2:\n",
    "                features = features.unsqueeze(0)  # Add batch dimension if missing\n",
    "            \n",
    "            recon, quantized, vq_loss, indices = model(features)\n",
    "            recon = recon.transpose(1, 2)  # Back to (batch_size, sequence_length, input_dim)\n",
    "            \n",
    "            # Normalize the loss by batch size\n",
    "            recon_loss = torch.nn.functional.mse_loss(recon, features, reduction='mean')\n",
    "            loss = recon_loss + vq_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track individual losses\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_vq_loss += vq_loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Print epoch statistics\n",
    "        avg_recon_loss = epoch_recon_loss / len(train_dataloader)\n",
    "        avg_vq_loss = epoch_vq_loss / len(train_dataloader)\n",
    "        avg_total_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Training - Recon Loss: {avg_recon_loss:.4f}, VQ Loss: {avg_vq_loss:.4f}, Total Loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_recon_loss = 0\n",
    "            test_vq_loss = 0\n",
    "            test_total_loss = 0\n",
    "            \n",
    "            for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "                features = batch.to(device)\n",
    "                if len(features.shape) == 2:\n",
    "                    features = features.unsqueeze(0)\n",
    "                    \n",
    "                recon, quantized, vq_loss, indices = model(features)\n",
    "                recon = recon.transpose(1, 2)\n",
    "                recon_loss = torch.nn.functional.mse_loss(recon, features, reduction='mean')\n",
    "                total_loss = recon_loss + vq_loss\n",
    "                \n",
    "                test_recon_loss += recon_loss.item()\n",
    "                test_vq_loss += vq_loss.item()\n",
    "                test_total_loss += total_loss.item()\n",
    "            \n",
    "            # Print test statistics\n",
    "            avg_test_recon_loss = test_recon_loss / len(test_dataloader)\n",
    "            avg_test_vq_loss = test_vq_loss / len(test_dataloader)\n",
    "            avg_test_total_loss = test_total_loss / len(test_dataloader)\n",
    "            print(f\"  Testing - Recon Loss: {avg_test_recon_loss:.4f}, VQ Loss: {avg_test_vq_loss:.4f}, Total Loss: {avg_test_total_loss:.4f}\\n\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1616635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/8\n",
      "  Training - Recon Loss: 1.8586, VQ Loss: 0.7210, Total Loss: 2.5796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.30it/s]\n",
      "Epochs:  12%|█▎        | 1/8 [00:08<00:59,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.5068, VQ Loss: 0.1194, Total Loss: 1.6262\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/8\n",
      "  Training - Recon Loss: 1.3822, VQ Loss: 0.1072, Total Loss: 1.4894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.44it/s]\n",
      "Epochs:  25%|██▌       | 2/8 [00:14<00:41,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.2460, VQ Loss: 0.1024, Total Loss: 1.3483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/8\n",
      "  Training - Recon Loss: 1.2031, VQ Loss: 0.0864, Total Loss: 1.2895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.49it/s]\n",
      "Epochs:  38%|███▊      | 3/8 [00:19<00:31,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.1380, VQ Loss: 0.0876, Total Loss: 1.2256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/8\n",
      "  Training - Recon Loss: 1.1317, VQ Loss: 0.0756, Total Loss: 1.2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.50it/s]\n",
      "Epochs:  50%|█████     | 4/8 [00:25<00:24,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.0954, VQ Loss: 0.0692, Total Loss: 1.1646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/8\n",
      "  Training - Recon Loss: 1.0930, VQ Loss: 0.0731, Total Loss: 1.1661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.49it/s]\n",
      "Epochs:  62%|██████▎   | 5/8 [00:31<00:17,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.0572, VQ Loss: 0.0741, Total Loss: 1.1314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/8\n",
      "  Training - Recon Loss: 1.0588, VQ Loss: 0.0723, Total Loss: 1.1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.47it/s]\n",
      "Epochs:  75%|███████▌  | 6/8 [00:37<00:11,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.0318, VQ Loss: 0.0659, Total Loss: 1.0978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/8\n",
      "  Training - Recon Loss: 1.0243, VQ Loss: 0.0616, Total Loss: 1.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.48it/s]\n",
      "Epochs:  88%|████████▊ | 7/8 [00:42<00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 1.0023, VQ Loss: 0.0514, Total Loss: 1.0537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/8\n",
      "  Training - Recon Loss: 1.0049, VQ Loss: 0.0569, Total Loss: 1.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  4.48it/s]\n",
      "Epochs: 100%|██████████| 8/8 [00:48<00:00,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing - Recon Loss: 0.9777, VQ Loss: 0.0560, Total Loss: 1.0337\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "model = train_model(model, train_dataloader, test_dataloader, num_epochs=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63397b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'vqvae_model_train2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e27d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "  Reconstruction Loss: 0.9472\n",
      "  VQ Loss: 0.0544\n",
      "  Total Loss: 1.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 39.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "  Reconstruction Loss: 0.9797\n",
      "  VQ Loss: 0.0543\n",
      "  Total Loss: 1.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a validation dataset (val_dataset) and corresponding dataloader (val_dataloader)\n",
    "# If not, you'll need to create one.  This example assumes you have one.\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_recon_loss = 0.0\n",
    "    total_vq_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            features = batch[0].to(device)  # Assuming your data loader returns a list, and the first element is your feature tensor\n",
    "            if len(features.shape) == 2:\n",
    "                features = features.unsqueeze(0)  # Add batch dimension if missing\n",
    "            recon, quantized, vq_loss, indices = model(features)\n",
    "            recon = recon.transpose(1, 2)\n",
    "            recon_loss = torch.nn.functional.mse_loss(recon, features, reduction='mean')\n",
    "            loss = recon_loss + vq_loss\n",
    "\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_vq_loss += vq_loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_recon_loss = total_recon_loss / num_batches\n",
    "    avg_vq_loss = total_vq_loss / num_batches\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"  Reconstruction Loss: {avg_recon_loss:.4f}\")\n",
    "    print(f\"  VQ Loss: {avg_vq_loss:.4f}\")\n",
    "    print(f\"  Total Loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "    return avg_recon_loss, avg_vq_loss, avg_total_loss\n",
    "\n",
    "# Example usage (assuming you have a validation dataloader named 'val_dataloader')\n",
    "# Make sure your model is on the correct device before evaluating\n",
    "device = 'mps'\n",
    "model = model.to(device)  # Ensure model is on the correct device\n",
    "\n",
    "recon_loss, vq_loss, total_loss = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "# You can also evaluate on the training data to check for overfitting:\n",
    "train_recon_loss, train_vq_loss, train_total_loss = evaluate_model(model, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "011ecc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'semantic_codec_final_20k_2.pth') #saved with state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57f059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
