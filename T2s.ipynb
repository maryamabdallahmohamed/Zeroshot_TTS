{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryamsaad/Documents/Zeroshot_TTS/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/maryamsaad/Documents/Zeroshot_TTS/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "from Vocab import Vocab\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from extract_semantics import load_semantic_extractor , extract_semantics\n",
    "from load_audio_features import extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Input Processing\n",
    "# The model processes three types of sequences concatenated together:\n",
    "# [TEXT_TOKENS] + [SEMANTIC_PROMPT] + [SEMANTIC_TARGET]\n",
    "# Sequence Structure\n",
    "\n",
    "# Text Tokens: Arabic text converted to token IDs\n",
    "# Semantic Prompt: A prefix of semantic tokens from reference audio\n",
    "# Semantic Target: The full semantic token sequence (masked during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_file</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>processed_fJ2vuI_700.mp3</td>\n",
       "      <td>مش بس المجاميع والناس اللي ورا اللي</td>\n",
       "      <td>[0.00993357878178358, 0.004596792161464691, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_htNK0t_9.mp3</td>\n",
       "      <td>والضغط والمذاكره ومشاريع التخرج وقرفها</td>\n",
       "      <td>[0.06607607007026672, 0.04131579399108887, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_0rMASI_336.mp3</td>\n",
       "      <td>بتاعه دواء يعني السكيزوفرينيا او الفصام</td>\n",
       "      <td>[0.021152175962924957, -0.040665335953235626, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 audio_file                               clean_text  \\\n",
       "0  processed_fJ2vuI_700.mp3      مش بس المجاميع والناس اللي ورا اللي   \n",
       "1    processed_htNK0t_9.mp3   والضغط والمذاكره ومشاريع التخرج وقرفها   \n",
       "2  processed_0rMASI_336.mp3  بتاعه دواء يعني السكيزوفرينيا او الفصام   \n",
       "\n",
       "                                      text_embedding  \n",
       "0  [0.00993357878178358, 0.004596792161464691, -0...  \n",
       "1  [0.06607607007026672, 0.04131579399108887, -0....  \n",
       "2  [0.021152175962924957, -0.040665335953235626, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"45k_embeddings.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Vocab()\n",
    "df['tokenized_prompts']=df['clean_text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Number of audio files: 5\n",
      "Loading Whisper model and processor...\n",
      "Extracting audio features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio Files: 100%|██████████| 5/5 [00:01<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Total feature sets: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_fJ2vuI_700.mp3 torch.Size([1500])\n"
     ]
    }
   ],
   "source": [
    "model = load_semantic_extractor(device='mps')\n",
    "semantic_tokens = []            # collect results\n",
    "for name, feat in features:     # feat is a tensor of shape (1, 1500, 512)\n",
    "    toks = extract_semantics(feat.to('mps'), model)   # <-- adds batch dim already (1,…)\n",
    "    semantic_tokens.append((name, toks.cpu()))\n",
    "\n",
    "print(semantic_tokens[0][0], semantic_tokens[0][1].shape)   # demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2S Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MaskGCTConfig:\n",
    "    vocab_size_text: int = 40000  # Text vocabulary size for Arabic\n",
    "    vocab_size_semantic: int = 1024  # Semantic token vocabulary size\n",
    "    max_seq_len: int = 2048\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 8\n",
    "    d_model: int = 512\n",
    "    d_ff: int = 1408  # 2.75 * d_model for GLU\n",
    "    dropout: float = 0.1\n",
    "    eps: float = 1e-5\n",
    "    theta: float = 10000.0  # RoPE theta\n",
    "    max_time_steps: int = 1000  # For diffusion scheduling\n",
    "    max_position_embeddings=1152\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRMSNorm(nn.Module):\n",
    "    \"\"\"Adaptive RMSNorm that accepts time step as condition\"\"\"\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        # Time conditioning MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model * 2)  # scale and shift\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, time_emb: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        # time_emb: (batch, d_model)\n",
    "        \n",
    "        # Get time-dependent scale and shift\n",
    "        time_out = self.time_mlp(time_emb)  # (batch, d_model * 2)\n",
    "        scale, shift = time_out.chunk(2, dim=-1)  # Each: (batch, d_model)\n",
    "        \n",
    "        # Apply RMSNorm\n",
    "        norm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        \n",
    "        # Apply time-dependent transformation\n",
    "        # Expand scale and shift to match x dimensions\n",
    "        scale = scale.unsqueeze(1)  # (batch, 1, d_model)\n",
    "        shift = shift.unsqueeze(1)  # (batch, 1, d_model)\n",
    "        \n",
    "        return norm * self.weight * (1 + scale) + shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n",
    "    def __init__(self, d_model: int,max_seq_len=2048, theta: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.theta = theta\n",
    "        \n",
    "        # Precompute frequencies\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute position encodings\n",
    "        t = torch.arange(max_seq_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        self.register_buffer('cos_cached', emb.cos())\n",
    "        self.register_buffer('sin_cached', emb.sin())\n",
    "    \n",
    "    def rotate_half(self, x):\n",
    "        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "        return torch.cat([-x2, x1], dim=-1)\n",
    "    \n",
    "    def forward(self, q, k, seq_len):\n",
    "        cos = self.cos_cached[:seq_len, :]\n",
    "        sin = self.sin_cached[:seq_len, :]\n",
    "        \n",
    "        # Apply rotary embedding to queries and keys\n",
    "        q_rot = q * cos + self.rotate_half(q) * sin\n",
    "        k_rot = k * cos + self.rotate_half(k) * sin\n",
    "        \n",
    "        return q_rot, k_rot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit(nn.Module):\n",
    "    \"\"\"Gated Linear Unit with GELU activation\"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate = F.gelu(self.gate_proj(x))\n",
    "        up = self.up_proj(x)\n",
    "        return self.down_proj(gate * up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "        \n",
    "        assert self.head_dim * self.n_heads == self.d_model, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        # Linear layers for q, k, v\n",
    "        self.q_proj = nn.Linear(self.d_model, self.d_model)\n",
    "        self.k_proj = nn.Linear(self.d_model, self.d_model)\n",
    "        self.v_proj = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "        # Initialize Rotary Positional Embedding with longer max_seq_len\n",
    "        self.rotary_emb = RotaryPositionalEmbedding(d_model=self.head_dim, max_seq_len=2048)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Project to q, k, v\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head attention: (batch, seq_len, n_heads, head_dim)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # (batch, heads, seq_len, head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply Rotary Positional Embedding to q and k\n",
    "        # Note: RoPE expects (batch, heads, seq_len, head_dim)\n",
    "        q, k = self.rotary_emb(q, k, seq_len)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch, heads, seq_len, seq_len)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, v)  # (batch, heads, seq_len, head_dim)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final output projection\n",
    "        out = self.out_proj(attn_output)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with bidirectional attention and GLU\"\"\"\n",
    "    def __init__(self, config: MaskGCTConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = GatedLinearUnit(config.d_model, config.d_ff)\n",
    "        self.norm1 = AdaptiveRMSNorm(config.d_model, config.eps)\n",
    "        self.norm2 = AdaptiveRMSNorm(config.d_model, config.eps)\n",
    "        \n",
    "    def forward(self, x, time_emb, attention_mask=None):\n",
    "        # Pre-norm attention\n",
    "        normed_x = self.norm1(x, time_emb)\n",
    "        attn_out = self.attention(normed_x, attention_mask)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # Pre-norm feed-forward\n",
    "        normed_x = self.norm2(x, time_emb)\n",
    "        ff_out = self.feed_forward(normed_x)\n",
    "        x = x + ff_out\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding for diffusion steps\"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, time_steps):\n",
    "        half_dim = self.d_model // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=time_steps.device) * -emb)\n",
    "        emb = time_steps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskGCT_T2S(nn.Module):\n",
    "    \"\"\"Text-to-Semantic MaskGCT Model\"\"\"\n",
    "    def __init__(self, config: MaskGCTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.text_embedding = nn.Embedding(config.vocab_size_text, config.d_model)\n",
    "        self.semantic_embedding = nn.Embedding(config.vocab_size_semantic, config.d_model)\n",
    "        \n",
    "        # Time embedding for diffusion\n",
    "        self.time_embedding = TimeEmbedding(config.d_model)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(config.d_model, config.d_model)\n",
    "        )\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output head\n",
    "        self.output_norm = AdaptiveRMSNorm(config.d_model, config.eps)\n",
    "        self.output_proj = nn.Linear(config.d_model, config.vocab_size_semantic)\n",
    "        \n",
    "        # Special tokens\n",
    "        self.mask_token_id = config.vocab_size_semantic - 1\n",
    "        self.pad_token_id = 0\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def create_attention_mask(self, text_len, prompt_len, target_len):\n",
    "        \"\"\"Create attention mask for [text, prompt, target] sequence\"\"\"\n",
    "        total_len = text_len + prompt_len + target_len\n",
    "        mask = torch.ones(total_len, total_len)\n",
    "        \n",
    "        # Text can attend to itself\n",
    "        mask[:text_len, :text_len] = 1\n",
    "        \n",
    "        # Prompt can attend to text and itself\n",
    "        mask[text_len:text_len+prompt_len, :text_len+prompt_len] = 1\n",
    "        \n",
    "        # Target can attend to text, prompt, and itself (bidirectional)\n",
    "        mask[text_len+prompt_len:, :] = 1\n",
    "        \n",
    "        return mask.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    def forward(self, \n",
    "                text_tokens: torch.Tensor,\n",
    "                semantic_prompt: torch.Tensor,\n",
    "                semantic_target: torch.Tensor,\n",
    "                time_steps: torch.Tensor,\n",
    "                mask_ratio: float = 0.15):\n",
    "        \"\"\"\n",
    "        Forward pass for training\n",
    "        \n",
    "        Args:\n",
    "            text_tokens: Text token sequence (batch, text_len)\n",
    "            semantic_prompt: Prompt semantic tokens (batch, prompt_len)\n",
    "            semantic_target: Target semantic tokens (batch, target_len)\n",
    "            time_steps: Diffusion time steps (batch,)\n",
    "            mask_ratio: Ratio of tokens to mask\n",
    "        \"\"\"\n",
    "        batch_size = text_tokens.shape[0]\n",
    "        text_len = text_tokens.shape[1]\n",
    "        prompt_len = semantic_prompt.shape[1]\n",
    "        target_len = semantic_target.shape[1]\n",
    "        \n",
    "        # Create masked target\n",
    "        masked_target = semantic_target.clone()\n",
    "        mask = torch.rand(batch_size, target_len) < mask_ratio\n",
    "        masked_target[mask] = self.mask_token_id\n",
    "        \n",
    "        # Embed tokens\n",
    "        text_emb = self.text_embedding(text_tokens)\n",
    "        prompt_emb = self.semantic_embedding(semantic_prompt)\n",
    "        target_emb = self.semantic_embedding(masked_target)\n",
    "        \n",
    "        # Concatenate sequences: [text, prompt, target]\n",
    "        x = torch.cat([text_emb, prompt_emb, target_emb], dim=1)\n",
    "        \n",
    "        # Time embedding\n",
    "        time_emb = self.time_embedding(time_steps)\n",
    "        time_emb = self.time_mlp(time_emb)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = self.create_attention_mask(text_len, prompt_len, target_len)\n",
    "        attention_mask = attention_mask.to(x.device)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, time_emb, attention_mask)\n",
    "        \n",
    "        # Apply output normalization and projection\n",
    "        x = self.output_norm(x, time_emb)\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        # Return only target logits\n",
    "        target_logits = logits[:, text_len + prompt_len:, :]\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    def generate(self,\n",
    "                 text_tokens: torch.Tensor,\n",
    "                 semantic_prompt: torch.Tensor,\n",
    "                 target_length: int,\n",
    "                 num_steps: int = 20,\n",
    "                 temperature: float = 1.0,\n",
    "                 top_k: int = None,\n",
    "                 top_p: float = None):\n",
    "        \"\"\"\n",
    "        Generate semantic tokens given text and prompt\n",
    "        \n",
    "        Args:\n",
    "            text_tokens: Text token sequence (batch, text_len)\n",
    "            semantic_prompt: Prompt semantic tokens (batch, prompt_len)\n",
    "            target_length: Length of target sequence to generate\n",
    "            num_steps: Number of denoising steps\n",
    "            temperature: Sampling temperature\n",
    "            top_k: Top-k sampling\n",
    "            top_p: Nucleus sampling\n",
    "        \"\"\"\n",
    "        batch_size = text_tokens.shape[0]\n",
    "        device = text_tokens.device\n",
    "        \n",
    "        # Initialize with all mask tokens\n",
    "        semantic_target = torch.full(\n",
    "            (batch_size, target_length), \n",
    "            self.mask_token_id, \n",
    "            device=device, \n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        # Iterative denoising\n",
    "        for step in range(num_steps):\n",
    "            # Current time step\n",
    "            t = torch.full((batch_size,), step / num_steps * self.config.max_time_steps, device=device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward_inference(text_tokens, semantic_prompt, semantic_target, t)\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Apply top-k and top-p filtering\n",
    "            if top_k is not None:\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "                logits = torch.full_like(logits, float('-inf'))\n",
    "                logits.scatter_(-1, top_k_indices, top_k_logits)\n",
    "            \n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                    -1, sorted_indices, sorted_indices_to_remove\n",
    "                )\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample tokens\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            new_tokens = torch.multinomial(probs.view(-1, probs.shape[-1]), 1).view(batch_size, target_length)\n",
    "            \n",
    "            # Update masked positions\n",
    "            mask_positions = (semantic_target == self.mask_token_id)\n",
    "            \n",
    "            # Schedule: unmask some tokens each step\n",
    "            num_to_unmask = max(1, int(mask_positions.sum().item() * (1 - (step + 1) / num_steps)))\n",
    "            \n",
    "            # Choose positions to unmask based on confidence\n",
    "            confidence = probs.max(dim=-1)[0]\n",
    "            confidence[~mask_positions] = -1  # Don't consider already unmasked positions\n",
    "            \n",
    "            # Get top confident positions to unmask\n",
    "            _, top_indices = torch.topk(confidence.view(batch_size, -1), num_to_unmask, dim=-1)\n",
    "            \n",
    "            # Update tokens\n",
    "            for b in range(batch_size):\n",
    "                for idx in top_indices[b]:\n",
    "                    if mask_positions[b, idx]:\n",
    "                        semantic_target[b, idx] = new_tokens[b, idx]\n",
    "        \n",
    "        return semantic_target\n",
    "    \n",
    "    def forward_inference(self, text_tokens, semantic_prompt, semantic_target, time_steps):\n",
    "        \"\"\"Forward pass for inference (no masking)\"\"\"\n",
    "        batch_size = text_tokens.shape[0]\n",
    "        text_len = text_tokens.shape[1]\n",
    "        prompt_len = semantic_prompt.shape[1]\n",
    "        target_len = semantic_target.shape[1]\n",
    "        \n",
    "        # Embed tokens\n",
    "        text_emb = self.text_embedding(text_tokens)\n",
    "        prompt_emb = self.semantic_embedding(semantic_prompt)\n",
    "        target_emb = self.semantic_embedding(semantic_target)\n",
    "        \n",
    "        # Concatenate sequences\n",
    "        x = torch.cat([text_emb, prompt_emb, target_emb], dim=1)\n",
    "        \n",
    "        # Time embedding\n",
    "        time_emb = self.time_embedding(time_steps)\n",
    "        time_emb = self.time_mlp(time_emb)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = self.create_attention_mask(text_len, prompt_len, target_len)\n",
    "        attention_mask = attention_mask.to(x.device)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, time_emb, attention_mask)\n",
    "        \n",
    "        # Apply output normalization and projection\n",
    "        x = self.output_norm(x, time_emb)\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        # Return only target logits\n",
    "        return logits[:, text_len + prompt_len:, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage and training setup\n",
    "def create_model_config():\n",
    "    \"\"\"Create configuration for Arabic TTS\"\"\"\n",
    "    return MaskGCTConfig(\n",
    "        vocab_size_text=100,  # Adjust based on your Arabic tokenizer\n",
    "        vocab_size_semantic=7500,  # Adjust based on your semantic codec\n",
    "        max_seq_len=1024,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        d_model=512,\n",
    "        d_ff=1408,\n",
    "        dropout=0.1,\n",
    "        eps=1e-5,\n",
    "        theta=10000.0,\n",
    "        max_time_steps=1000\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "def training_step(model, batch, optimizer, criterion):\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    text_tokens = batch['text_tokens']  # (batch, text_len)\n",
    "    semantic_prompt = batch['semantic_prompt']  # (batch, prompt_len)\n",
    "    semantic_target = batch['semantic_target']  # (batch, target_len)\n",
    "    \n",
    "    # Random time steps\n",
    "    batch_size = text_tokens.shape[0]\n",
    "    time_steps = torch.randint(0, model.config.max_time_steps, (batch_size,), device=text_tokens.device)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(text_tokens, semantic_prompt, semantic_target, time_steps)\n",
    "    \n",
    "    # Compute loss (cross-entropy with masked positions)\n",
    "    loss = criterion(logits.reshape(-1, logits.shape[-1]), semantic_target.reshape(-1))\n",
    "\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextSemanticDataset(Dataset):\n",
    "    def __init__(self, df, semantic_tokens_list, max_semantic_len=1000, min_prompt_len=5, max_prompt_len=50):\n",
    "        \"\"\"\n",
    "        df: pandas DataFrame with 'tokenized_prompts' column (list of tokens)\n",
    "        semantic_tokens_list: list of semantic tokens tensors (squeezed 1D LongTensor)\n",
    "        \"\"\"\n",
    "        assert len(df) == len(semantic_tokens_list), \"Mismatch between text and semantic tokens length\"\n",
    "\n",
    "        self.text_tokens = df['tokenized_prompts'].tolist()\n",
    "        self.semantic_tokens = semantic_tokens_list\n",
    "        self.max_semantic_len = max_semantic_len\n",
    "        self.min_prompt_len = min_prompt_len\n",
    "        self.max_prompt_len = max_prompt_len\n",
    "\n",
    "    def random_prefix(self, semantic_tokens):\n",
    "        if semantic_tokens.shape[0] > self.max_semantic_len:\n",
    "            semantic_tokens = semantic_tokens[:self.max_semantic_len]\n",
    "\n",
    "        prompt_len = min(\n",
    "            max(self.min_prompt_len, semantic_tokens.shape[0] // 4),\n",
    "            min(self.max_prompt_len, semantic_tokens.shape[0] - 1)\n",
    "        )\n",
    "\n",
    "        semantic_prompt = semantic_tokens[:prompt_len]\n",
    "        semantic_target = semantic_tokens\n",
    "\n",
    "        max_target_len = 1024 - prompt_len\n",
    "        semantic_prompt = semantic_tokens[:prompt_len]\n",
    "        semantic_target = semantic_tokens[:max_target_len]\n",
    "\n",
    "        \n",
    "        return semantic_prompt, semantic_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.LongTensor(self.text_tokens[idx])\n",
    "        \n",
    "        name, semantic_tensor = self.semantic_tokens[idx]  # <-- Unpack tuple\n",
    "        if semantic_tensor.dim() == 2:\n",
    "            semantic_tensor = semantic_tensor.squeeze(0)\n",
    "\n",
    "        semantic_prompt, semantic_target = self.random_prefix(semantic_tensor)\n",
    "\n",
    "        return {\n",
    "            'text_tokens': text,\n",
    "            'semantic_prompt': semantic_prompt,\n",
    "            'semantic_target': semantic_target,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch is a list of dicts with 'text_tokens', 'semantic_prompt', 'semantic_target'\n",
    "    max_text_len = max(item['text_tokens'].size(0) for item in batch)\n",
    "    max_prompt_len = max(item['semantic_prompt'].size(0) for item in batch)\n",
    "    max_target_len = max(item['semantic_target'].size(0) for item in batch)\n",
    "\n",
    "    max_text_len = min(max_text_len, 2048)\n",
    "    max_prompt_len = min(max_prompt_len, 2048)\n",
    "    max_target_len = min(max_target_len, 2048)\n",
    "\n",
    "    # pad each tensor to max length in batch (or max 2048)\n",
    "    text_tokens = torch.stack([torch.nn.functional.pad(item['text_tokens'], (0, max_text_len - item['text_tokens'].size(0))) for item in batch])\n",
    "    semantic_prompt = torch.stack([torch.nn.functional.pad(item['semantic_prompt'], (0, max_prompt_len - item['semantic_prompt'].size(0))) for item in batch])\n",
    "    semantic_target = torch.stack([torch.nn.functional.pad(item['semantic_target'], (0, max_target_len - item['semantic_target'].size(0))) for item in batch])\n",
    "\n",
    "    return {\n",
    "        'text_tokens': text_tokens,\n",
    "        'semantic_prompt': semantic_prompt,\n",
    "        'semantic_target': semantic_target,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextSemanticDataset at 0x3b165cd00>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextSemanticDataset(df_filtered,semantic_tokens)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=create_model_config()\n",
    "model=MaskGCT_T2S(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup\n",
    "model = MaskGCT_T2S(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "total_loss = 0.0\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "    # Move batch to device\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key]\n",
    "    \n",
    "    # Training step\n",
    "    loss = training_step(model, batch, optimizer, criterion)\n",
    "    total_loss += loss\n",
    "    \n",
    "    # Optionally: print every N batches\n",
    "    if (batch_idx + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        tqdm.write(f\"Batch {batch_idx + 1}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'T2S.pth') #saved with state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
