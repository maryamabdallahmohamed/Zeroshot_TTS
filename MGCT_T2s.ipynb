{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11944092,"sourceType":"datasetVersion","datasetId":7494136}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import shutil\nshutil.copy(\"/kaggle/input/tts-dataset/Vocab.py\", \"/kaggle/working\")\nshutil.copy(\"/kaggle/input/tts-dataset/extract_semantics.py\", \"/kaggle/working\")\nshutil.copy(\"/kaggle/input/tts-dataset/load_audio_features.py\", \"/kaggle/working\")\nshutil.copy(\"/kaggle/input/tts-dataset/semantic_codec_final_20k_2.pth\", \"/kaggle/working\")\n\nimport sys\nsys.path.append('/kaggle/working')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:44:51.007431Z","iopub.execute_input":"2025-05-25T15:44:51.007702Z","iopub.status.idle":"2025-05-25T15:44:52.130292Z","shell.execute_reply.started":"2025-05-25T15:44:51.007681Z","shell.execute_reply":"2025-05-25T15:44:52.129713Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom typing import Optional, Tuple, List\nfrom dataclasses import dataclass\nfrom Vocab import Vocab\nimport pandas as pd\nimport librosa\nfrom extract_semantics import load_semantic_extractor , extract_semantics\nfrom load_audio_features import get_all_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:44:52.131348Z","iopub.execute_input":"2025-05-25T15:44:52.131595Z","iopub.status.idle":"2025-05-25T15:45:32.884320Z","shell.execute_reply.started":"2025-05-25T15:44:52.131566Z","shell.execute_reply":"2025-05-25T15:45:32.883748Z"}},"outputs":[{"name":"stderr","text":"2025-05-25 15:45:03.627661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748187904.106693      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748187904.241373      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# 2. Input Processing\n# The model processes three types of sequences concatenated together:\n# [TEXT_TOKENS] + [SEMANTIC_PROMPT] + [SEMANTIC_TARGET]\n# Sequence Structure\n\n# Text Tokens: Arabic text converted to token IDs\n# Semantic Prompt: A prefix of semantic tokens from reference audio\n# Semantic Target: The full semantic token sequence (masked during training)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:45:32.885066Z","iopub.execute_input":"2025-05-25T15:45:32.885561Z","iopub.status.idle":"2025-05-25T15:45:32.889118Z","shell.execute_reply.started":"2025-05-25T15:45:32.885531Z","shell.execute_reply":"2025-05-25T15:45:32.888389Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/tts-dataset/45k_embeddings.csv\")\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:45:32.891024Z","iopub.execute_input":"2025-05-25T15:45:32.891261Z","iopub.status.idle":"2025-05-25T15:45:49.030620Z","shell.execute_reply.started":"2025-05-25T15:45:32.891245Z","shell.execute_reply":"2025-05-25T15:45:49.029967Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 audio_file                               clean_text  \\\n0  processed_fJ2vuI_700.mp3      مش بس المجاميع والناس اللي ورا اللي   \n1    processed_htNK0t_9.mp3   والضغط والمذاكره ومشاريع التخرج وقرفها   \n2  processed_0rMASI_336.mp3  بتاعه دواء يعني السكيزوفرينيا او الفصام   \n\n                                      text_embedding  \n0  [0.00993357878178358, 0.004596792161464691, -0...  \n1  [0.06607607007026672, 0.04131579399108887, -0....  \n2  [0.021152175962924957, -0.040665335953235626, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_file</th>\n      <th>clean_text</th>\n      <th>text_embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>processed_fJ2vuI_700.mp3</td>\n      <td>مش بس المجاميع والناس اللي ورا اللي</td>\n      <td>[0.00993357878178358, 0.004596792161464691, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>processed_htNK0t_9.mp3</td>\n      <td>والضغط والمذاكره ومشاريع التخرج وقرفها</td>\n      <td>[0.06607607007026672, 0.04131579399108887, -0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>processed_0rMASI_336.mp3</td>\n      <td>بتاعه دواء يعني السكيزوفرينيا او الفصام</td>\n      <td>[0.021152175962924957, -0.040665335953235626, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"tokenizer=Vocab()\ndf['tokenized_prompts']=df['clean_text'].apply(tokenizer.tokenize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:45:49.031344Z","iopub.execute_input":"2025-05-25T15:45:49.031657Z","iopub.status.idle":"2025-05-25T15:45:49.343409Z","shell.execute_reply.started":"2025-05-25T15:45:49.031639Z","shell.execute_reply":"2025-05-25T15:45:49.342578Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"features = get_all_features() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:45:49.344594Z","iopub.execute_input":"2025-05-25T15:45:49.344860Z","iopub.status.idle":"2025-05-25T16:25:03.107682Z","shell.execute_reply.started":"2025-05-25T15:45:49.344843Z","shell.execute_reply":"2025-05-25T16:25:03.107050Z"}},"outputs":[{"name":"stderr","text":"Validating audio files: 100%|██████████| 8000/8000 [02:16<00:00, 58.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ valid audio files found: 8000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d343bff5f504625b6687221e4a572bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e034e7a9cb254713926120efd75b761a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1fae7d33ea44955ba9ec3530f1cbf68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481c4fcab2fc4ca386374cb057fc437d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a9ea3c572d47c8941aa458979b3995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14fbb1fea0541169fd261e2417fc77f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb24e7004ab5455c9937f7ccedee433f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f387f01692647baa5e1a6726d106f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51ae06e20f6441bba5481b5b8c0094d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba1115c118348fbb8579095fb7f1124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c765abd3fd246ffbddfff16e3774182"}},"metadata":{}},{"name":"stderr","text":"Extracting Whisper features: 100%|██████████| 4000/4000 [36:52<00:00,  1.81it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Transpose(nn.Module):\n    def __init__(self, dim0=1, dim1=2):\n        super().__init__()\n        self.dim0 = dim0\n        self.dim1 = dim1\n\n    def forward(self, x):\n        return x.transpose(self.dim0, self.dim1)\n\n\nclass SemanticExtractor(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=384, codebook_size=8192, codebook_dim=8):\n        super().__init__()\n        # Only keep the encoder and quantizer parts\n        self.encoder = nn.Sequential(\n            nn.Conv1d(input_dim, hidden_dim, kernel_size=7, padding=3),\n            Transpose(),                             # (B, 384, L) → (B, L, 384)\n            nn.LayerNorm(hidden_dim),               # now normalizes the last dim\n            Transpose(),                             # (B, L, 384) → (B, 384, L)\n            *[ConvNextBlock(hidden_dim) for _ in range(6)],\n            nn.Conv1d(hidden_dim, codebook_dim, kernel_size=1)\n        )\n\n        self.quantizer = VectorQuantizer(num_embeddings=codebook_size, embedding_dim=codebook_dim)\n\n    def forward(self, x):\n        # x shape: (batch_size, sequence_length, input_dim)\n        x = x.transpose(1, 2)  # To (batch_size, input_dim, sequence_length)\n        z = self.encoder(x)  # Get encoded features\n        z = z.transpose(1, 2)  # To (batch_size, sequence_length, codebook_dim)\n        _, _, indices = self.quantizer(z)  # Get semantic tokens\n        return indices\n\nclass ConvNextBlock(nn.Module):\n    def __init__(self, dim, kernel_size=7):\n        super().__init__()\n        self.conv = nn.Conv1d(dim, dim, kernel_size, padding=kernel_size//2, groups=dim)  # Depthwise\n        self.norm = nn.LayerNorm(dim)\n        self.pwconv1 = nn.Linear(dim, 4 * dim)\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        self.act = nn.GELU()\n\n    def forward(self, x):\n        # x: (batch, dim, seq_len)\n        residual = x\n        x = self.conv(x)\n        # Transpose for LayerNorm\n        x = x.transpose(1, 2)  # (batch, seq_len, dim)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        # Transpose back\n        x = x.transpose(1, 2)  # (batch, dim, seq_len)\n        return x + residual\n\n# Vector Quantization Layer\nclass VectorQuantizer(nn.Module):\n    def __init__(self, num_embeddings=8192, embedding_dim=8, commitment_cost=0.25):\n        \n        super().__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.commitment_cost = commitment_cost\n        self.embeddings = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n        self.register_buffer('ema_count', torch.zeros(num_embeddings))\n        self.register_buffer('ema_weight', self.embeddings.clone())\n\n    def forward(self, x):\n\n        flat_x = x.reshape(-1, self.embedding_dim)\n        distances = torch.cdist(flat_x, self.embeddings)\n        encoding_indices = torch.argmin(distances, dim=1)\n        quantized = self.embeddings[encoding_indices].reshape(x.shape)\n        codebook_loss = F.mse_loss(quantized.detach(), x)\n        commitment_loss = self.commitment_cost * F.mse_loss(quantized, x.detach())\n        loss = codebook_loss + commitment_loss\n        quantized = x + (quantized - x).detach()\n        \n        if self.training:\n            with torch.no_grad():\n                one_hot = F.one_hot(encoding_indices, self.num_embeddings).float()\n                self.ema_count = 0.999 * self.ema_count + 0.001 * torch.sum(one_hot, dim=0)\n                n = torch.sum(self.ema_count)\n                self.ema_count = (self.ema_count + 1e-8) / (n + self.num_embeddings * 1e-8) * n\n                dw = torch.matmul(one_hot.transpose(0, 1), flat_x)\n                self.ema_weight = 0.999 * self.ema_weight + 0.001 * dw\n                self.embeddings.data = (self.ema_weight / (self.ema_count.unsqueeze(-1) + 1e-8))\n        \n        return quantized, loss, encoding_indices\n\ndef load_semantic_extractor(model_path='/kaggle/working/semantic_codec_final_20k_2.pth', device='cuda'):\n    print(device)\n    model = SemanticExtractor(input_dim=512, hidden_dim=384, codebook_size=8192, codebook_dim=8)\n    state_dict = torch.load(model_path, map_location=device)\n \n\n    # Load encoder weights\n    encoder_state = {k.replace(\"encoder.\", \"\"): v for k, v in state_dict.items() if k.startswith(\"encoder.\")}\n    model.encoder.load_state_dict(encoder_state, strict=False)  # <=== set strict=False\n\n    # Load quantizer weights\n    quantizer_state = {k.replace(\"quantizer.\", \"\"): v for k, v in state_dict.items() if k.startswith(\"quantizer.\")}\n    model.quantizer.load_state_dict(quantizer_state, strict=False)  \n\n    model.to(device)\n    model.eval()\n    return model\n\n\ndef extract_semantics(audio_features, model):\n    \"\"\"\n    Extract semantic tokens from audio features\n    Args:\n        audio_features: Tensor of shape (batch_size, sequence_length, 512)\n        model: Loaded SemanticExtractor model\n    Returns:\n        semantic_tokens: Tensor of shape (batch_size, sequence_length)\n    \"\"\"\n    with torch.no_grad():\n        semantic_tokens = model(audio_features)\n    return semantic_tokens\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.389890Z","iopub.status.idle":"2025-05-25T16:26:35.390287Z","shell.execute_reply.started":"2025-05-25T16:26:35.390141Z","shell.execute_reply":"2025-05-25T16:26:35.390157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from extract_semantics import load_semantic_extractor , extract_semantics\nmodel = load_semantic_extractor(device='cuda')\nsemantic_tokens = []     \naudio_files=[]       # collect results\nfor name, feat in features:\n    if len(feat.shape) == 2:\n        feat = feat.unsqueeze(0)  # Add batch dimension if missing\n    audio_files.append(name)\n    toks = extract_semantics(feat.to('cuda'), model)   # <-- adds batch dim already (1,…)\n    semantic_tokens.append(toks.squeeze()) #|Remove batch dimension before appending\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.391469Z","iopub.status.idle":"2025-05-25T16:26:35.391803Z","shell.execute_reply.started":"2025-05-25T16:26:35.391608Z","shell.execute_reply":"2025-05-25T16:26:35.391624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf=df[df['audio_file'].isin(audio_files)]\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.392583Z","iopub.status.idle":"2025-05-25T16:26:35.392841Z","shell.execute_reply.started":"2025-05-25T16:26:35.392697Z","shell.execute_reply":"2025-05-25T16:26:35.392706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"semantic_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.393560Z","iopub.status.idle":"2025-05-25T16:26:35.393834Z","shell.execute_reply.started":"2025-05-25T16:26:35.393702Z","shell.execute_reply":"2025-05-25T16:26:35.393715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## T2S Arch","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass MaskGCTConfig:\n    vocab_size_text: int = 40000  # Text vocabulary size for Arabic\n    vocab_size_semantic: int = 1024  # Semantic token vocabulary size\n    max_seq_len: int = 2048\n    n_layers: int = 12\n    n_heads: int = 4\n    d_model: int = 512\n    d_ff: int = 1408  # 2.75 * d_model for GLU\n    dropout: float = 0.1\n    eps: float = 1e-5\n    theta: float = 10000.0  # RoPE theta\n    max_time_steps: int = 1000  # For diffusion scheduling\n    max_position_embeddings=1152\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.394498Z","iopub.status.idle":"2025-05-25T16:26:35.394792Z","shell.execute_reply.started":"2025-05-25T16:26:35.394643Z","shell.execute_reply":"2025-05-25T16:26:35.394657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AdaptiveRMSNorm(nn.Module):\n    \"\"\"Adaptive RMSNorm that accepts time step as condition\"\"\"\n    def __init__(self, d_model: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n        # Time conditioning MLP\n        self.time_mlp = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.SiLU(),\n            nn.Linear(d_model, d_model * 2)  # scale and shift\n        )\n        \n    def forward(self, x: torch.Tensor, time_emb: torch.Tensor) -> torch.Tensor:\n        # x: (batch, seq_len, d_model)\n        # time_emb: (batch, d_model)\n        \n        # Get time-dependent scale and shift\n        time_out = self.time_mlp(time_emb)  # (batch, d_model * 2)\n        scale, shift = time_out.chunk(2, dim=-1)  # Each: (batch, d_model)\n        \n        # Apply RMSNorm\n        norm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n        \n        # Apply time-dependent transformation\n        # Expand scale and shift to match x dimensions\n        scale = scale.unsqueeze(1)  # (batch, 1, d_model)\n        shift = shift.unsqueeze(1)  # (batch, 1, d_model)\n        \n        return norm * self.weight * (1 + scale) + shift\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.395751Z","iopub.status.idle":"2025-05-25T16:26:35.396055Z","shell.execute_reply.started":"2025-05-25T16:26:35.395927Z","shell.execute_reply":"2025-05-25T16:26:35.395939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RotaryPositionalEmbedding(nn.Module):\n    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n    def __init__(self, d_model: int,max_seq_len=2048, theta: float = 10000.0):\n        super().__init__()\n        self.d_model = d_model\n        self.theta = theta\n        \n        # Precompute frequencies\n        inv_freq = 1.0 / (theta ** (torch.arange(0, d_model, 2).float() / d_model))\n        self.register_buffer('inv_freq', inv_freq)\n        \n        # Precompute position encodings\n        t = torch.arange(max_seq_len).type_as(inv_freq)\n        freqs = torch.einsum('i,j->ij', t, inv_freq)\n        emb = torch.cat([freqs, freqs], dim=-1)\n        self.register_buffer('cos_cached', emb.cos())\n        self.register_buffer('sin_cached', emb.sin())\n    \n    def rotate_half(self, x):\n        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n        return torch.cat([-x2, x1], dim=-1)\n    \n    def forward(self, q, k, seq_len):\n        cos = self.cos_cached[:seq_len, :]\n        sin = self.sin_cached[:seq_len, :]\n        \n        # Apply rotary embedding to queries and keys\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n        \n        return q_rot, k_rot\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.397183Z","iopub.status.idle":"2025-05-25T16:26:35.397409Z","shell.execute_reply.started":"2025-05-25T16:26:35.397305Z","shell.execute_reply":"2025-05-25T16:26:35.397314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GatedLinearUnit(nn.Module):\n    \"\"\"Gated Linear Unit with GELU activation\"\"\"\n    def __init__(self, d_model: int, d_ff: int):\n        super().__init__()\n        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n    \n    def forward(self, x):\n        gate = F.gelu(self.gate_proj(x))\n        up = self.up_proj(x)\n        return self.down_proj(gate * up)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.398301Z","iopub.status.idle":"2025-05-25T16:26:35.398618Z","shell.execute_reply.started":"2025-05-25T16:26:35.398461Z","shell.execute_reply":"2025-05-25T16:26:35.398474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.d_model = config.d_model\n        self.n_heads = config.n_heads\n        self.head_dim = self.d_model // self.n_heads\n        \n        assert self.head_dim * self.n_heads == self.d_model, \"d_model must be divisible by n_heads\"\n\n        # Linear layers for q, k, v\n        self.q_proj = nn.Linear(self.d_model, self.d_model)\n        self.k_proj = nn.Linear(self.d_model, self.d_model)\n        self.v_proj = nn.Linear(self.d_model, self.d_model)\n        \n        self.out_proj = nn.Linear(self.d_model, self.d_model)\n        \n        # Initialize Rotary Positional Embedding with longer max_seq_len\n        self.rotary_emb = RotaryPositionalEmbedding(d_model=self.head_dim, max_seq_len=2048)\n\n    def forward(self, x, attention_mask=None):\n        batch_size, seq_len, _ = x.size()\n\n        # Project to q, k, v\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n        \n        # Reshape for multi-head attention: (batch, seq_len, n_heads, head_dim)\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # (batch, heads, seq_len, head_dim)\n        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        \n        # Apply Rotary Positional Embedding to q and k\n        # Note: RoPE expects (batch, heads, seq_len, head_dim)\n        q, k = self.rotary_emb(q, k, seq_len)\n        \n        # Scaled dot-product attention\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch, heads, seq_len, seq_len)\n        \n        if attention_mask is not None:\n            attn_scores = attn_scores.masked_fill(attention_mask == 0, float('-inf'))\n        \n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_probs, v)  # (batch, heads, seq_len, head_dim)\n        \n        # Concatenate heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        \n        # Final output projection\n        out = self.out_proj(attn_output)\n        \n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.399775Z","iopub.status.idle":"2025-05-25T16:26:35.400122Z","shell.execute_reply.started":"2025-05-25T16:26:35.399941Z","shell.execute_reply":"2025-05-25T16:26:35.399954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"Transformer block with bidirectional attention and GLU\"\"\"\n    def __init__(self, config: MaskGCTConfig):\n        super().__init__()\n        self.attention = MultiHeadAttention(config)\n        self.feed_forward = GatedLinearUnit(config.d_model, config.d_ff)\n        self.norm1 = AdaptiveRMSNorm(config.d_model, config.eps)\n        self.norm2 = AdaptiveRMSNorm(config.d_model, config.eps)\n        \n    def forward(self, x, time_emb, attention_mask=None):\n        # Pre-norm attention\n        normed_x = self.norm1(x, time_emb)\n        attn_out = self.attention(normed_x, attention_mask)\n        x = x + attn_out\n        \n        # Pre-norm feed-forward\n        normed_x = self.norm2(x, time_emb)\n        ff_out = self.feed_forward(normed_x)\n        x = x + ff_out\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.401029Z","iopub.status.idle":"2025-05-25T16:26:35.401352Z","shell.execute_reply.started":"2025-05-25T16:26:35.401172Z","shell.execute_reply":"2025-05-25T16:26:35.401188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TimeEmbedding(nn.Module):\n    \"\"\"Sinusoidal time embedding for diffusion steps\"\"\"\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n        \n    def forward(self, time_steps):\n        half_dim = self.d_model // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=time_steps.device) * -emb)\n        emb = time_steps[:, None] * emb[None, :]\n        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n        return emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.402274Z","iopub.status.idle":"2025-05-25T16:26:35.402499Z","shell.execute_reply.started":"2025-05-25T16:26:35.402390Z","shell.execute_reply":"2025-05-25T16:26:35.402400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaskGCT_T2S(nn.Module):\n    \"\"\"Text-to-Semantic MaskGCT Model\"\"\"\n    def __init__(self, config: MaskGCTConfig):\n        super().__init__()\n        self.config = config\n        \n        # Embeddings\n        self.text_embedding = nn.Embedding(config.vocab_size_text, config.d_model)\n        self.semantic_embedding = nn.Embedding(config.vocab_size_semantic, config.d_model)\n        \n        # Time embedding for diffusion\n        self.time_embedding = TimeEmbedding(config.d_model)\n        self.time_mlp = nn.Sequential(\n            nn.Linear(config.d_model, config.d_model),\n            nn.SiLU(),\n            nn.Linear(config.d_model, config.d_model)\n        )\n        \n        # Transformer layers\n        self.layers = nn.ModuleList([\n            TransformerBlock(config) for _ in range(config.n_layers)\n        ])\n        \n        # Output head\n        self.output_norm = AdaptiveRMSNorm(config.d_model, config.eps)\n        self.output_proj = nn.Linear(config.d_model, config.vocab_size_semantic)\n        \n        # Special tokens\n        self.mask_token_id = config.vocab_size_semantic - 1\n        self.pad_token_id = 0\n        \n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n    \n    def create_attention_mask(self, text_len, prompt_len, target_len):\n        \"\"\"Create attention mask for [text, prompt, target] sequence\"\"\"\n        total_len = text_len + prompt_len + target_len\n        mask = torch.ones(total_len, total_len)\n        \n        # Text can attend to itself\n        mask[:text_len, :text_len] = 1\n        \n        # Prompt can attend to text and itself\n        mask[text_len:text_len+prompt_len, :text_len+prompt_len] = 1\n        \n        # Target can attend to text, prompt, and itself (bidirectional)\n        mask[text_len+prompt_len:, :] = 1\n        \n        return mask.unsqueeze(0)  # Add batch dimension\n    \n    def forward(self, \n                text_tokens: torch.Tensor,\n                semantic_prompt: torch.Tensor,\n                semantic_target: torch.Tensor,\n                time_steps: torch.Tensor,\n                mask_ratio: float = 0.15):\n        \"\"\"\n        Forward pass for training\n        \n        Args:\n            text_tokens: Text token sequence (batch, text_len)\n            semantic_prompt: Prompt semantic tokens (batch, prompt_len)\n            semantic_target: Target semantic tokens (batch, target_len)\n            time_steps: Diffusion time steps (batch,)\n            mask_ratio: Ratio of tokens to mask\n        \"\"\"\n        batch_size = text_tokens.shape[0]\n        text_len = text_tokens.shape[1]\n        prompt_len = semantic_prompt.shape[1]\n        target_len = semantic_target.shape[1]\n        \n        # Create masked target\n        masked_target = semantic_target.clone()\n        mask = torch.rand(batch_size, target_len) < mask_ratio\n        masked_target[mask] = self.mask_token_id\n        \n        # Embed tokens\n        text_emb = self.text_embedding(text_tokens)\n        prompt_emb = self.semantic_embedding(semantic_prompt)\n        target_emb = self.semantic_embedding(masked_target)\n        \n        # Concatenate sequences: [text, prompt, target]\n        x = torch.cat([text_emb, prompt_emb, target_emb], dim=1)\n        \n        # Time embedding\n        time_emb = self.time_embedding(time_steps)\n        time_emb = self.time_mlp(time_emb)\n        \n        # Create attention mask\n        attention_mask = self.create_attention_mask(text_len, prompt_len, target_len)\n        attention_mask = attention_mask.to(x.device)\n        \n        # Apply transformer layers\n        for layer in self.layers:\n            x = layer(x, time_emb, attention_mask)\n        \n        # Apply output normalization and projection\n        x = self.output_norm(x, time_emb)\n        logits = self.output_proj(x)\n        \n        # Return only target logits\n        target_logits = logits[:, text_len + prompt_len:, :]\n        \n        return target_logits\n    \n    def generate(self,\n                 text_tokens: torch.Tensor,\n                 semantic_prompt: torch.Tensor,\n                 target_length: int,\n                 num_steps: int = 20,\n                 temperature: float = 1.0,\n                 top_k: int = None,\n                 top_p: float = None):\n        \"\"\"\n        Generate semantic tokens given text and prompt\n        \n        Args:\n            text_tokens: Text token sequence (batch, text_len)\n            semantic_prompt: Prompt semantic tokens (batch, prompt_len)\n            target_length: Length of target sequence to generate\n            num_steps: Number of denoising steps\n            temperature: Sampling temperature\n            top_k: Top-k sampling\n            top_p: Nucleus sampling\n        \"\"\"\n        batch_size = text_tokens.shape[0]\n        device = text_tokens.device\n        \n        # Initialize with all mask tokens\n        semantic_target = torch.full(\n            (batch_size, target_length), \n            self.mask_token_id, \n            device=device, \n            dtype=torch.long\n        )\n        \n        # Iterative denoising\n        for step in range(num_steps):\n            # Current time step\n            t = torch.full((batch_size,), step / num_steps * self.config.max_time_steps, device=device)\n            \n            # Forward pass\n            with torch.no_grad():\n                logits = self.forward_inference(text_tokens, semantic_prompt, semantic_target, t)\n            \n            # Apply temperature\n            logits = logits / temperature\n            \n            # Apply top-k and top-p filtering\n            if top_k is not None:\n                top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n                logits = torch.full_like(logits, float('-inf'))\n                logits.scatter_(-1, top_k_indices, top_k_logits)\n            \n            if top_p is not None:\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n                \n                indices_to_remove = sorted_indices_to_remove.scatter(\n                    -1, sorted_indices, sorted_indices_to_remove\n                )\n                logits[indices_to_remove] = float('-inf')\n            \n            # Sample tokens\n            probs = F.softmax(logits, dim=-1)\n            new_tokens = torch.multinomial(probs.view(-1, probs.shape[-1]), 1).view(batch_size, target_length)\n            \n            # Update masked positions\n            mask_positions = (semantic_target == self.mask_token_id)\n            \n            # Schedule: unmask some tokens each step\n            num_to_unmask = max(1, int(mask_positions.sum().item() * (1 - (step + 1) / num_steps)))\n            \n            # Choose positions to unmask based on confidence\n            confidence = probs.max(dim=-1)[0]\n            confidence[~mask_positions] = -1  # Don't consider already unmasked positions\n            \n            # Get top confident positions to unmask\n            _, top_indices = torch.topk(confidence.view(batch_size, -1), num_to_unmask, dim=-1)\n            \n            # Update tokens\n            for b in range(batch_size):\n                for idx in top_indices[b]:\n                    if mask_positions[b, idx]:\n                        semantic_target[b, idx] = new_tokens[b, idx]\n        \n        return semantic_target\n    \n    def forward_inference(self, text_tokens, semantic_prompt, semantic_target, time_steps):\n        \"\"\"Forward pass for inference (no masking)\"\"\"\n        batch_size = text_tokens.shape[0]\n        text_len = text_tokens.shape[1]\n        prompt_len = semantic_prompt.shape[1]\n        target_len = semantic_target.shape[1]\n        \n        # Embed tokens\n        text_emb = self.text_embedding(text_tokens)\n        prompt_emb = self.semantic_embedding(semantic_prompt)\n        target_emb = self.semantic_embedding(semantic_target)\n        \n        # Concatenate sequences\n        x = torch.cat([text_emb, prompt_emb, target_emb], dim=1)\n        \n        # Time embedding\n        time_emb = self.time_embedding(time_steps)\n        time_emb = self.time_mlp(time_emb)\n        \n        # Create attention mask\n        attention_mask = self.create_attention_mask(text_len, prompt_len, target_len)\n        attention_mask = attention_mask.to(x.device)\n        \n        # Apply transformer layers\n        for layer in self.layers:\n            x = layer(x, time_emb, attention_mask)\n        \n        # Apply output normalization and projection\n        x = self.output_norm(x, time_emb)\n        logits = self.output_proj(x)\n        \n        # Return only target logits\n        return logits[:, text_len + prompt_len:, :]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:40.441154Z","iopub.execute_input":"2025-05-25T16:26:40.441401Z","iopub.status.idle":"2025-05-25T16:26:40.462921Z","shell.execute_reply.started":"2025-05-25T16:26:40.441384Z","shell.execute_reply":"2025-05-25T16:26:40.462172Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"\n# Example usage and training setup\ndef create_model_config():\n    \"\"\"Create configuration for Arabic TTS\"\"\"\n    return MaskGCTConfig(\n        vocab_size_text=100,  # Adjust based on your Arabic tokenizer\n        vocab_size_semantic=7500,  # Adjust based on your semantic codec\n        max_seq_len=1024,\n        n_layers=6,\n        n_heads=2,\n        d_model=512,\n        d_ff=1408,\n        dropout=0.1,\n        eps=1e-5,\n        theta=10000.0,\n        max_time_steps=1000\n)\n    \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:40.698817Z","iopub.execute_input":"2025-05-25T16:27:40.699479Z","iopub.status.idle":"2025-05-25T16:27:40.704334Z","shell.execute_reply.started":"2025-05-25T16:27:40.699448Z","shell.execute_reply":"2025-05-25T16:27:40.703619Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## Dataset and Dataloaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass TextSemanticDataset(Dataset):\n    def __init__(self, df, semantic_tokens_list, max_semantic_len=1000, min_prompt_len=5, max_prompt_len=50):\n        \"\"\"\n        df: pandas DataFrame with 'tokenized_prompts' column (list of tokens)\n        semantic_tokens_list: list of semantic tokens tensors (squeezed 1D LongTensor)\n        \"\"\"\n        assert len(df) == len(semantic_tokens_list), \"Mismatch between text and semantic tokens length\"\n\n        self.text_tokens = df['tokenized_prompts'].tolist()\n        self.semantic_tokens = semantic_tokens_list\n        self.max_semantic_len = max_semantic_len\n        self.min_prompt_len = min_prompt_len\n        self.max_prompt_len = max_prompt_len\n\n    def random_prefix(self, semantic_tokens):\n        if semantic_tokens.shape[0] > self.max_semantic_len:\n            semantic_tokens = semantic_tokens[:self.max_semantic_len]\n\n        prompt_len = min(\n            max(self.min_prompt_len, semantic_tokens.shape[0] // 4),\n            min(self.max_prompt_len, semantic_tokens.shape[0] - 1)\n        )\n\n        semantic_prompt = semantic_tokens[:prompt_len]\n        semantic_target = semantic_tokens\n\n        max_target_len = 1024 - prompt_len\n        semantic_prompt = semantic_tokens[:prompt_len]\n        semantic_target = semantic_tokens[:max_target_len]\n\n        \n        return semantic_prompt, semantic_target\n\n    def __len__(self):\n        return len(self.text_tokens)\n\n    def __getitem__(self, idx):\n        text = torch.LongTensor(self.text_tokens[idx])\n        \n        name, semantic_tensor = self.semantic_tokens[idx]  # <-- Unpack tuple\n        if semantic_tensor.dim() == 2:\n            semantic_tensor = semantic_tensor.squeeze(0)\n\n        semantic_prompt, semantic_target = self.random_prefix(semantic_tensor)\n\n        return {\n            'text_tokens': text,\n            'semantic_prompt': semantic_prompt,\n            'semantic_target': semantic_target,\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:41.017214Z","iopub.execute_input":"2025-05-25T16:27:41.017449Z","iopub.status.idle":"2025-05-25T16:27:41.024540Z","shell.execute_reply.started":"2025-05-25T16:27:41.017434Z","shell.execute_reply":"2025-05-25T16:27:41.023904Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"class TextSemanticDataset(Dataset):\n    def __init__(self, df, semantic_tokens_list, max_semantic_len=1000, min_prompt_len=5, max_prompt_len=50):\n        \"\"\"\n        df: pandas DataFrame with 'tokenized_prompts' column (list of tokens)\n        semantic_tokens_list: list of semantic tokens tensors (squeezed 1D LongTensor)\n        \"\"\"\n        assert len(df) == len(semantic_tokens_list), \"Mismatch between text and semantic tokens length\"\n        \n        self.text_tokens = df['tokenized_prompts'].tolist()\n        self.semantic_tokens = semantic_tokens_list  # This should be a list of tensors, not tuples\n        self.max_semantic_len = max_semantic_len\n        self.min_prompt_len = min_prompt_len\n        self.max_prompt_len = max_prompt_len\n\n    def __len__(self):\n        return len(self.text_tokens)\n\n    def __getitem__(self, idx):\n        text = torch.LongTensor(self.text_tokens[idx])\n        \n        # Get semantic tensor directly, no need to unpack\n        semantic_tensor = self.semantic_tokens[idx]\n        \n        # Ensure semantic tensor is 1D\n        if semantic_tensor.dim() == 2:\n            semantic_tensor = semantic_tensor.squeeze(0)\n        \n        # Truncate if too long\n        if len(semantic_tensor) > self.max_semantic_len:\n            semantic_tensor = semantic_tensor[:self.max_semantic_len]\n        \n        # Truncate text if too long\n        if len(text) > self.max_prompt_len:\n            text = text[:self.max_prompt_len]\n        \n        return {\n            'text': text,\n            'semantic': semantic_tensor\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:41.115848Z","iopub.execute_input":"2025-05-25T16:27:41.116261Z","iopub.status.idle":"2025-05-25T16:27:41.121873Z","shell.execute_reply.started":"2025-05-25T16:27:41.116244Z","shell.execute_reply":"2025-05-25T16:27:41.121191Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"class TextSemanticDataset(Dataset):\n    def __init__(self, df, semantic_tokens_list, max_semantic_len=1000, min_prompt_len=5, max_prompt_len=50):\n        \"\"\"\n        df: pandas DataFrame with 'tokenized_prompts' column (list of tokens)\n        semantic_tokens_list: list of semantic tokens tensors (squeezed 1D LongTensor)\n        \"\"\"\n        assert len(df) == len(semantic_tokens_list), \"Mismatch between text and semantic tokens length\"\n        \n        self.text_tokens = df['tokenized_prompts'].tolist()\n        self.semantic_tokens = semantic_tokens_list  # List of tensors\n        self.max_semantic_len = max_semantic_len\n        self.min_prompt_len = min_prompt_len\n        self.max_prompt_len = max_prompt_len\n\n    def random_prefix(self, semantic_tokens):\n        if semantic_tokens.shape[0] > self.max_semantic_len:\n            semantic_tokens = semantic_tokens[:self.max_semantic_len]\n\n        prompt_len = min(\n            max(self.min_prompt_len, semantic_tokens.shape[0] // 4),\n            min(self.max_prompt_len, semantic_tokens.shape[0] - 1)\n        )\n\n        semantic_prompt = semantic_tokens[:prompt_len]\n        semantic_target = semantic_tokens\n\n        max_target_len = 1024 - prompt_len\n        semantic_prompt = semantic_tokens[:prompt_len]\n        semantic_target = semantic_tokens[:max_target_len]\n\n        return semantic_prompt, semantic_target\n\n    def __len__(self):\n        return len(self.text_tokens)\n\n    def __getitem__(self, idx):\n        # Get text tokens\n        text = torch.LongTensor(self.text_tokens[idx])\n        \n        # Get semantic tokens (already a tensor)\n        semantic_tensor = self.semantic_tokens[idx]\n        \n        # Ensure semantic tensor is 1D\n        if semantic_tensor.dim() == 2:\n            semantic_tensor = semantic_tensor.squeeze(0)\n        \n        # Get prompt and target\n        semantic_prompt, semantic_target = self.random_prefix(semantic_tensor)\n\n        return {\n            'text_tokens': text,\n            'semantic_prompt': semantic_prompt,\n            'semantic_target': semantic_target,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:41.185531Z","iopub.execute_input":"2025-05-25T16:27:41.186021Z","iopub.status.idle":"2025-05-25T16:27:41.192600Z","shell.execute_reply.started":"2025-05-25T16:27:41.185973Z","shell.execute_reply":"2025-05-25T16:27:41.191963Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def collate_fn(batch):\n    # batch is a list of dicts with 'text_tokens', 'semantic_prompt', 'semantic_target'\n    max_text_len = max(item['text_tokens'].size(0) for item in batch)\n    max_prompt_len = max(item['semantic_prompt'].size(0) for item in batch)\n    max_target_len = max(item['semantic_target'].size(0) for item in batch)\n\n    max_text_len = min(max_text_len, 2048)\n    max_prompt_len = min(max_prompt_len, 2048)\n    max_target_len = min(max_target_len, 2048)\n\n    # pad each tensor to max length in batch (or max 2048)\n    text_tokens = torch.stack([torch.nn.functional.pad(item['text_tokens'], (0, max_text_len - item['text_tokens'].size(0))) for item in batch])\n    semantic_prompt = torch.stack([torch.nn.functional.pad(item['semantic_prompt'], (0, max_prompt_len - item['semantic_prompt'].size(0))) for item in batch])\n    semantic_target = torch.stack([torch.nn.functional.pad(item['semantic_target'], (0, max_target_len - item['semantic_target'].size(0))) for item in batch])\n\n    return {\n        'text_tokens': text_tokens,\n        'semantic_prompt': semantic_prompt,\n        'semantic_target': semantic_target,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:41.478547Z","iopub.execute_input":"2025-05-25T16:27:41.478810Z","iopub.status.idle":"2025-05-25T16:27:41.485251Z","shell.execute_reply.started":"2025-05-25T16:27:41.478791Z","shell.execute_reply":"2025-05-25T16:27:41.484555Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"filtered_df=df[df['audio_file'].isin(audio_files)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:45.360637Z","iopub.execute_input":"2025-05-25T16:27:45.361043Z","iopub.status.idle":"2025-05-25T16:27:45.371305Z","shell.execute_reply.started":"2025-05-25T16:27:45.360989Z","shell.execute_reply":"2025-05-25T16:27:45.370695Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# 1. Create a quick lookup map from audio file names to their semantic tokens\naudio_to_semantic_map = {name: token for name, token in zip(audio_files, semantic_tokens)}\n\n# 2. Use the audio file names from your *filtered* DataFrame to get the relevant tokens\nfiltered_semantic_tokens = [audio_to_semantic_map[name] for name in filtered_df['audio_file'].tolist()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:45.705786Z","iopub.execute_input":"2025-05-25T16:27:45.706083Z","iopub.status.idle":"2025-05-25T16:27:45.713379Z","shell.execute_reply.started":"2025-05-25T16:27:45.706062Z","shell.execute_reply":"2025-05-25T16:27:45.712619Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"df.shape\ndf.columns\nprint(len(filtered_df))\nprint(len(filtered_semantic_tokens))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:47.472304Z","iopub.execute_input":"2025-05-25T16:27:47.472563Z","iopub.status.idle":"2025-05-25T16:27:47.476925Z","shell.execute_reply.started":"2025-05-25T16:27:47.472543Z","shell.execute_reply":"2025-05-25T16:27:47.476189Z"}},"outputs":[{"name":"stdout","text":"7666\n7666\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch\n# Now create the dataset\ndataset = TextSemanticDataset(filtered_df,filtered_semantic_tokens)\n\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:47.898839Z","iopub.execute_input":"2025-05-25T16:27:47.899422Z","iopub.status.idle":"2025-05-25T16:27:47.905019Z","shell.execute_reply.started":"2025-05-25T16:27:47.899400Z","shell.execute_reply":"2025-05-25T16:27:47.904285Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"<__main__.TextSemanticDataset at 0x7fa1e0798050>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch\n\n# Assuming you have your dataset ready\n# Split dataset into train and test (80-20 split)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create separate dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:48.073589Z","iopub.execute_input":"2025-05-25T16:27:48.074116Z","iopub.status.idle":"2025-05-25T16:27:48.080617Z","shell.execute_reply.started":"2025-05-25T16:27:48.074094Z","shell.execute_reply":"2025-05-25T16:27:48.079860Z"}},"outputs":[{"name":"stdout","text":"Train dataset size: 6132\nTest dataset size: 1534\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"config=create_model_config()\nmodel = MaskGCT_T2S(config).to('cuda')\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\ndevice='cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:49.933958Z","iopub.execute_input":"2025-05-25T16:27:49.934660Z","iopub.status.idle":"2025-05-25T16:27:50.517381Z","shell.execute_reply.started":"2025-05-25T16:27:49.934636Z","shell.execute_reply":"2025-05-25T16:27:50.516815Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def training_step(model, batch, optimizer, criterion):\n    \"\"\"Single training step\"\"\"\n    text_tokens = batch['text_tokens']  # (batch, text_len)\n    semantic_prompt = batch['semantic_prompt']  # (batch, prompt_len)\n    semantic_target = batch['semantic_target']  # (batch, target_len)\n    \n    # Random time steps\n    batch_size = text_tokens.shape[0]\n    time_steps = torch.randint(0, model.config.max_time_steps, (batch_size,), device=text_tokens.device)\n    \n    # Forward pass\n    logits = model(text_tokens, semantic_prompt, semantic_target, time_steps)\n    \n    # Compute loss (cross-entropy with masked positions)\n    loss = criterion(logits.reshape(-1, logits.shape[-1]), semantic_target.reshape(-1))\n\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    return loss.item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:27:55.164573Z","iopub.execute_input":"2025-05-25T16:27:55.165144Z","iopub.status.idle":"2025-05-25T16:27:55.169866Z","shell.execute_reply.started":"2025-05-25T16:27:55.165124Z","shell.execute_reply":"2025-05-25T16:27:55.169086Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn as nn\n\n# Setup\nmodel = MaskGCT_T2S(config)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\ndevice = torch.device(\"cuda\")\nmodel.to(device)\n\n# Training loop\nnum_epochs = 5  # Define the number of epochs\nmodel.train()\n\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n    \n    with tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n        for batch_idx, batch in enumerate(pbar):\n            # Move batch to device\n            for key in batch:\n                batch[key] = batch[key].to(device)\n            \n            # Training step\n            loss = training_step(model, batch, optimizer, criterion)\n            total_loss += loss  # Accumulate the loss\n            \n            # Optionally: print every N batches\n            if (batch_idx + 1) % 10 == 0:\n                avg_loss = total_loss / (batch_idx + 1)\n                pbar.set_postfix({\"Avg Loss\": avg_loss}) # Update tqdm progress bar\n                \n    # Print epoch summary\n    epoch_avg_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs} - Avg Loss: {epoch_avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:28:23.003641Z","iopub.execute_input":"2025-05-25T16:28:23.004326Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1533/1533 [08:23<00:00,  3.04it/s, Avg Loss=0.0469]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Avg Loss: 0.0469\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1533/1533 [08:23<00:00,  3.04it/s, Avg Loss=0.01]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 - Avg Loss: 0.0100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1533/1533 [08:23<00:00,  3.04it/s, Avg Loss=0.00949]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 - Avg Loss: 0.0095\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:   7%|▋         | 106/1533 [00:34<07:47,  3.05it/s, Avg Loss=0.0093]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'T2S.pth') #saved with state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:26:35.388529Z","iopub.status.idle":"2025-05-25T16:26:35.388848Z","shell.execute_reply.started":"2025-05-25T16:26:35.388682Z","shell.execute_reply":"2025-05-25T16:26:35.388696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}